{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_JIT_USE_NNC_NOT_NVFUSER\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Optional, Union, Iterable\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Compose\n",
    "from e3nn import o3\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from diffusion_edf.embedding import NodeEmbeddingNetwork\n",
    "from diffusion_edf.data import SE3, PointCloud, TargetPoseDemo, DemoSequence, DemoSeqDataset, load_demos, save_demos\n",
    "from diffusion_edf.preprocess import Rescale, NormalizeColor, Downsample, PointJitter, ColorJitter\n",
    "from diffusion_edf.wigner import TransformFeatureQuaternion\n",
    "from diffusion_edf.score_model import ScoreModel\n",
    "from diffusion_edf import transforms\n",
    "from diffusion_edf.loss import SE3DenoisingDiffusion\n",
    "from diffusion_edf.utils import sample_reference_points\n",
    "from diffusion_edf.dist import diffuse_isotropic_se3\n",
    "\n",
    "\n",
    "torch.set_printoptions(precision=4, sci_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_len = 0.01\n",
    "scene_voxel_size = 0.01\n",
    "grasp_voxel_size = 0.01\n",
    "\n",
    "scene_voxel_size = scene_voxel_size / unit_len\n",
    "grasp_voxel_size = grasp_voxel_size / unit_len\n",
    "\n",
    "\n",
    "rescale_fn = Rescale(rescale_factor=1/unit_len)\n",
    "recover_scale_fn = Rescale(rescale_factor=unit_len)\n",
    "normalize_color_fn = NormalizeColor(color_mean = torch.tensor([0.5, 0.5, 0.5]), color_std = torch.tensor([0.5, 0.5, 0.5]))\n",
    "recover_color_fn = NormalizeColor(color_mean = -normalize_color_fn.color_mean / normalize_color_fn.color_std, color_std = 1 / normalize_color_fn.color_std)\n",
    "\n",
    "\n",
    "scene_proc_fn = Compose([rescale_fn,\n",
    "                         Downsample(voxel_size=scene_voxel_size, coord_reduction=\"average\"),\n",
    "                         normalize_color_fn])\n",
    "scene_unproc_fn = Compose([recover_color_fn, recover_scale_fn])\n",
    "grasp_proc_fn = Compose([rescale_fn,\n",
    "                         Downsample(voxel_size=grasp_voxel_size, coord_reduction=\"average\"),\n",
    "                         normalize_color_fn])\n",
    "grasp_unproc_fn = Compose([recover_color_fn, recover_scale_fn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "device = 'cuda:0'\n",
    "compile = False\n",
    "\n",
    "irreps_input = o3.Irreps('3x0e')\n",
    "irreps_node_embedding = o3.Irreps('32x0e+16x1e+8x2e') #o3.Irreps('128x0e+64x1e+32x2e')\n",
    "irreps_sh = o3.Irreps('1x0e+1x1e+1x2e')\n",
    "fc_neurons = [128, 64, 64]\n",
    "num_heads = 4\n",
    "alpha_drop = 0.2\n",
    "proj_drop = 0.0\n",
    "drop_path_rate = 0.0\n",
    "irreps_mlp_mid = 2\n",
    "n_scales = 4\n",
    "pool_ratio = 0.5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = DemoSeqDataset(dataset_dir=\"demo/test_demo\", annotation_file=\"data.yaml\", device=device)\n",
    "train_dataloader = DataLoader(trainset, shuffle=True, collate_fn=lambda x:x)\n",
    "#writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 200\n",
    "N_samples = 10\n",
    "\n",
    "iter_ = 0\n",
    "loss_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusion_edf.pc_utils import get_plotly_fig\n",
    "\n",
    "\n",
    "def get_raw_pointcloud(**kwargs) -> Tuple[PointCloud, PointCloud]:\n",
    "\n",
    "    ################### Write your custom codes here ###################\n",
    "    dir, idx, pick_or_place = kwargs['dir'], kwargs['idx'], kwargs['pick_or_place']\n",
    "\n",
    "    demos = DemoSeqDataset(dataset_dir=\"demo/test_demo\", annotation_file=\"data.yaml\")\n",
    "    demo: DemoSequence = demos[idx]\n",
    "    if pick_or_place == 'pick':\n",
    "        demo: TargetPoseDemo = demo[0]\n",
    "    elif pick_or_place == 'place':\n",
    "        demo: TargetPoseDemo = demo[1]\n",
    "    else:\n",
    "        raise ValueError(f\"Wrong value for pick_or_place argument: {pick_or_place}\")\n",
    "\n",
    "    scene_pcd: PointCloud = demo.scene_pc\n",
    "    grasp_pcd: PointCloud = demo.grasp_pc\n",
    "    target_pose: SE3 = demo.target_poses\n",
    "    ####################################################################\n",
    "\n",
    "    return scene_pcd, grasp_pcd\n",
    "\n",
    "\n",
    "def visualize(scene_pcd: PointCloud, grasp_pcd: PointCloud, poses: SE3, query: Optional[Tuple[torch.Tensor, torch.Tensor]] = None, show_sample_points: bool = False):\n",
    "    \n",
    "    grasp_pl = grasp_pcd.plotly(point_size=1.0, name=\"grasp\")\n",
    "    grasp_geometry = [grasp_pl]\n",
    "    if query is not None:\n",
    "        query_points, query_attention = query\n",
    "        query_opacity = query_attention ** 1\n",
    "        query_pl = PointCloud.points_to_plotly(pcd=query_points, point_size=15.0, opacity=query_opacity / query_opacity.max())#, custom_data={'attention': query_attention.cpu()})\n",
    "        grasp_geometry.append(query_pl)\n",
    "    fig_grasp = get_plotly_fig(\"Grasp\")\n",
    "    fig_grasp = fig_grasp.add_traces(grasp_geometry)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    scene_pl = scene_pcd.plotly(point_size=1.0, name='scene')\n",
    "    placement_geometry = [scene_pl]\n",
    "    transformed_grasp_pcd = grasp_pcd.transformed(poses)\n",
    "    for i in range(len(poses)):\n",
    "        pose_pl = transformed_grasp_pcd[i].plotly(point_size=1.0, name=f'pose_{i}')\n",
    "        placement_geometry.append(pose_pl)\n",
    "    if show_sample_points:\n",
    "        sample_pl = PointCloud.points_to_plotly(pcd=poses.points, point_size=7.0, colors=[0.2, 0.5, 0.8], name=f'sample_points')\n",
    "        placement_geometry.append(sample_pl)\n",
    "    fig_sample = get_plotly_fig(\"Sampled Placement\")\n",
    "    fig_sample = fig_sample.add_traces(placement_geometry)\n",
    "    \n",
    "    trace_dict = {}\n",
    "    visiblility_list = []\n",
    "    for i, trace in enumerate(fig_sample.data):\n",
    "        trace_dict[trace.name] = i\n",
    "        if trace.name[:4] == 'pose':\n",
    "            trace.visible = False\n",
    "            visiblility_list.append(False)\n",
    "        else:\n",
    "            visiblility_list.append(trace.visible)\n",
    "\n",
    "    # Define sliders\n",
    "    steps = []\n",
    "    for i in range(len(poses)):\n",
    "        step = dict(\n",
    "            method=\"update\",\n",
    "            args=[{\"visible\": visiblility_list.copy()},\n",
    "                {\"title\": \"Visualizing pose_\" + str(i)}],  # layout attribute\n",
    "        )\n",
    "        step[\"args\"][0][\"visible\"][trace_dict[f'pose_{i}']] = True  # Toggle i'th trace to \"visible\"\n",
    "        steps.append(step)\n",
    "\n",
    "    sliders = [dict(\n",
    "        active=0,\n",
    "        currentvalue={\"prefix\": \"Pose: \"},\n",
    "        pad={\"t\": 50},\n",
    "        steps=steps\n",
    "    )]\n",
    "\n",
    "    fig_sample.update_layout(\n",
    "        sliders=sliders\n",
    "    )\n",
    "\n",
    "    fig_sample.data[trace_dict[f'pose_0']].visible = True\n",
    "\n",
    "\n",
    "\n",
    "    return fig_grasp, fig_sample\n",
    "\n",
    "def reverse_diffusion(T, score, timestep, angular_first = True):\n",
    "    if angular_first:\n",
    "        score = torch.cat([score[..., 3:], score[..., :3]], dim=-1)\n",
    "\n",
    "    score = score * timestep # Flow\n",
    "    #score = (score * timestep / 2) + (torch.randn_like(score) * torch.sqrt(timestep)) # Langevin\n",
    "    reverse_T = transforms.se3_exp_map(score)\n",
    "    reverse_T = torch.cat([transforms.standardize_quaternion(transforms.matrix_to_quaternion(reverse_T[..., :3, :3])), reverse_T[..., :3, -1]], dim=-1)\n",
    "\n",
    "    T = torch.cat([transforms.quaternion_multiply(T[..., :4], reverse_T[..., :4]), \n",
    "                   transforms.quaternion_apply(T[..., :4], reverse_T[..., 4:]) + T[..., 4:]], dim=-1)\n",
    "    # T = torch.cat([quaternion_multiply(reverse_T[..., :4], T[..., :4]), \n",
    "    #                quaternion_apply(reverse_T[..., :4], T[..., 4:]) + reverse_T[..., 4:]], dim=-1)\n",
    "    \n",
    "    return T, reverse_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch = next(iter(train_dataloader))\n",
    "\n",
    "demo_seq: DemoSequence = train_batch[0]\n",
    "demo: TargetPoseDemo = demo_seq[1]\n",
    "\n",
    "scene_raw: PointCloud = demo.scene_pc\n",
    "grasp_raw: PointCloud = demo.grasp_pc\n",
    "target_poses: SE3 = demo.target_poses\n",
    "\n",
    "scene_proc = scene_proc_fn(scene_raw).to(device)\n",
    "grasp_proc = grasp_proc_fn(grasp_raw).to(device)\n",
    "target_poses = rescale_fn(target_poses).to(device)\n",
    "T_target = target_poses.poses\n",
    "\n",
    "\n",
    "x_ref, n_neighbors = sample_reference_points(PointCloud.transform_pcd(scene_proc, target_poses.inv())[0].points, grasp_proc.points, r=3)\n",
    "print(n_neighbors.max().item(), n_neighbors.sum().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_in = 0.3\n",
    "lin_mult = 1.\n",
    "std = time_in * lin_mult\n",
    "eps = time_in / 2\n",
    "T, target_score = diffuse_isotropic_se3(T0 = T_target, eps=eps, std=std, N=1, angular_first=True, double_precision=True)\n",
    "T = T.squeeze(0)\n",
    "target_score = target_score.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_feature = scene_proc.colors\n",
    "key_coord = scene_proc.points\n",
    "key_batch = torch.zeros(len(key_coord), device=device, dtype=torch.long)\n",
    "query_feature = grasp_proc.colors\n",
    "query_coord = grasp_proc.points\n",
    "query_batch = torch.zeros(len(query_coord), device=device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_rev, dT = reverse_diffusion(T, target_score, timestep=time_in, angular_first=True)\n",
    "#T_rev, dT = reverse_diffusion(T, score, timestep=time_in, angular_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_pose = recover_scale_fn(SE3(T_target))[:1]\n",
    "fig_grasp, fig_sample = visualize(scene_pcd=scene_raw, grasp_pcd=grasp_raw, poses=vis_pose, query=(x_ref.cpu()*unit_len, torch.tensor([1.,]))) #, query=(edf_outputs['query_points'] * unit_len, edf_outputs['query_attention']))\n",
    "fig_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_grasp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vis_pose = recover_scale_fn(SE3(T_target))[:1]\n",
    "vis_pose = recover_scale_fn(SE3(T))[:1]\n",
    "#vis_pose = recover_scale_fn(SE3(T_rev))[:1]\n",
    "fig_grasp, fig_sample = visualize(scene_pcd=scene_raw, grasp_pcd=grasp_raw, poses=vis_pose) #, query=(edf_outputs['query_points'] * unit_len, edf_outputs['query_attention']))\n",
    "fig_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asfd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_score * time_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vis_pose = recover_scale_fn(SE3(T_target))[:1]\n",
    "#vis_pose = recover_scale_fn(SE3(T))[:1]\n",
    "vis_pose = recover_scale_fn(SE3(T_rev))[:1]\n",
    "fig_grasp, fig_sample = visualize(scene_pcd=scene_raw, grasp_pcd=grasp_raw, poses=vis_pose) #, query=(edf_outputs['query_points'] * unit_len, edf_outputs['query_attention']))\n",
    "# fig_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "math.sin(0.32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diff_edf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "79a0085b6cf04e1cff261ad12d41cff4e1530d9e68d1f8fc6bd159a2915452c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
