{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Tuple, Optional, Union\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import torch\n",
    "from torchvision.transforms import Compose\n",
    "\n",
    "from e3nn import o3\n",
    "from e3nn.util.jit import compile_mode, script as e3nn_script\n",
    "\n",
    "from diffusion_edf.embedding import NodeEmbeddingNetwork\n",
    "from diffusion_edf.data import SE3, PointCloud, TargetPoseDemo, DemoSequence, DemoSeqDataset, load_demos, save_demos\n",
    "from diffusion_edf.preprocess import Rescale, NormalizeColor, Downsample, PointJitter, ColorJitter\n",
    "from diffusion_edf.wigner import TransformFeatureQuaternion\n",
    "from diffusion_edf.block import PoolingBlock, EquiformerBlock, DownBlock, EdfExtractor\n",
    "from diffusion_edf.connectivity import RadiusGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_len = 0.01\n",
    "scene_voxel_size = 0.01\n",
    "grasp_voxel_size = 0.01\n",
    "\n",
    "scene_voxel_size = scene_voxel_size / unit_len\n",
    "grasp_voxel_size = grasp_voxel_size / unit_len\n",
    "\n",
    "\n",
    "rescale_fn = Rescale(rescale_factor=1/unit_len)\n",
    "recover_scale_fn = Rescale(rescale_factor=unit_len)\n",
    "normalize_color_fn = NormalizeColor(color_mean = torch.tensor([0.5, 0.5, 0.5]), color_std = torch.tensor([0.5, 0.5, 0.5]))\n",
    "recover_color_fn = NormalizeColor(color_mean = -normalize_color_fn.color_mean / normalize_color_fn.color_std, color_std = 1 / normalize_color_fn.color_std)\n",
    "\n",
    "\n",
    "scene_proc_fn = Compose([rescale_fn,\n",
    "                         Downsample(voxel_size=scene_voxel_size, coord_reduction=\"average\"),\n",
    "                         normalize_color_fn])\n",
    "scene_unproc_fn = Compose([recover_color_fn, recover_scale_fn])\n",
    "grasp_proc_fn = Compose([rescale_fn,\n",
    "                         Downsample(voxel_size=grasp_voxel_size, coord_reduction=\"average\"),\n",
    "                         normalize_color_fn])\n",
    "grasp_unproc_fn = Compose([recover_color_fn, recover_scale_fn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hw/anaconda3/envs/diff_edf/lib/python3.8/site-packages/torch/jit/_check.py:181: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\"The TorchScript type system doesn't support \"\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda:0'\n",
    "compile = True\n",
    "eval = True\n",
    "\n",
    "irreps_input = o3.Irreps('3x0e')\n",
    "irreps_node_embedding = o3.Irreps('64x0e+32x1e+16x2e') #o3.Irreps('128x0e+64x1e+32x2e')\n",
    "irreps_sh = o3.Irreps('1x0e+1x1e+1x2e')\n",
    "irreps_block_output = o3.Irreps('64x0e+32x1e+16x2e') #o3.Irreps('128x0e+64x1e+32x2e')\n",
    "number_of_basis = 128\n",
    "fc_neurons = [128, 64, 64]\n",
    "irreps_head = o3.Irreps('16x0e+8x1e+4x2e') #o3.Irreps('32x0e+16x1o+8x2e')\n",
    "num_heads = 4\n",
    "irreps_pre_attn = None\n",
    "rescale_degree = False\n",
    "nonlinear_message = True\n",
    "alpha_drop = 0.2\n",
    "proj_drop = 0.0\n",
    "drop_path_rate = 0.0\n",
    "irreps_mlp_mid = 2\n",
    "norm_layer = 'layer'\n",
    "n_scales = 4\n",
    "\n",
    "\n",
    "\n",
    "node_enc = NodeEmbeddingNetwork(irreps_input=irreps_input, irreps_node_emb=irreps_node_embedding)\n",
    "down_block = DownBlock(irreps = irreps_node_embedding,\n",
    "                       irreps_edge_attr = irreps_sh,\n",
    "                       irreps_head = irreps_head,\n",
    "                       num_heads = num_heads,\n",
    "                       fc_neurons = fc_neurons,\n",
    "                       init_radius = 2.0,\n",
    "                       pool_ratio = 0.5,\n",
    "                       n_scales = n_scales,\n",
    "                       n_layers_per_scale = 2,\n",
    "                       pool_method = 'fps',\n",
    "                       deterministic = True,\n",
    "                       irreps_mlp_mid = irreps_mlp_mid,\n",
    "                       attn_type='mlp',\n",
    "                       alpha_drop=alpha_drop, \n",
    "                       proj_drop=proj_drop,\n",
    "                       drop_path_rate=drop_path_rate)\n",
    "\n",
    "if compile:\n",
    "    node_enc = e3nn_script(node_enc).to(device)\n",
    "    down_block = e3nn_script(down_block).to(device)\n",
    "else:\n",
    "    node_enc = node_enc.to(device)\n",
    "    down_block = down_block.to(device)\n",
    "\n",
    "if eval:\n",
    "    node_enc.eval()\n",
    "    down_block.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = EdfExtractor(\n",
    "    irreps_inputs = [o3.Irreps('64x0e+32x1e+16x2e') for _ in range(n_scales)],\n",
    "    fc_neurons_inputs = [[64, 32, 32] for _ in range(n_scales)],\n",
    "    irreps_emb = irreps_node_embedding,\n",
    "    irreps_edge_attr = irreps_sh,\n",
    "    irreps_head = irreps_head,\n",
    "    num_heads = num_heads,\n",
    "    fc_neurons = fc_neurons,\n",
    "    n_layers = 1,\n",
    "    cutoffs = [2., 3.5, 5., 7.5],\n",
    "    offsets = [0.01, 0.01, 0.01, 0.01],\n",
    "    query_radius = 8.,\n",
    "    irreps_mlp_mid = irreps_mlp_mid,\n",
    "    attn_type='mlp',\n",
    "    alpha_drop=alpha_drop, \n",
    "    proj_drop=proj_drop,\n",
    "    drop_path_rate=drop_path_rate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = extractor.to(device)\n",
    "extractor.eval()\n",
    "pass"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TargetPoseDemo  (name: pick_demo)\n"
     ]
    }
   ],
   "source": [
    "demo_list: List[DemoSequence] = load_demos(dir='demo/test_demo')\n",
    "demo_seq: DemoSequence = demo_list[0]\n",
    "\n",
    "demo: TargetPoseDemo = demo_seq[0]\n",
    "print(demo)\n",
    "\n",
    "scene_raw: PointCloud = demo.scene_pc\n",
    "grasp_raw: PointCloud = demo.grasp_pc\n",
    "target_poses: TargetPoseDemo = demo.target_poses\n",
    "\n",
    "scene_proc = scene_proc_fn(scene_raw).to(device)\n",
    "grasp_proc = grasp_proc_fn(grasp_raw).to(device)\n",
    "\n",
    "#print(scene_proc)\n",
    "#go.Figure(scene_unproc_fn(scene_proc).plotly())\n",
    "\n",
    "\n",
    "node_feature = scene_proc.colors\n",
    "node_coord = scene_proc.points\n",
    "batch = torch.zeros(len(node_coord), device=device, dtype=torch.long)\n",
    "\n",
    "query_points = torch.randn(3,3, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_emb = node_enc(node_feature)\n",
    "outputs = down_block(node_feature=node_emb,\n",
    "                     node_coord=node_coord,\n",
    "                     batch=batch)\n",
    "\n",
    "field_val = extractor(query_coord = query_points, \n",
    "                      query_batch = torch.zeros_like(query_points[...,0]),\n",
    "                      node_features = [output[0] for output in outputs[::2]],\n",
    "                      node_coords = [output[1] for output in outputs[::2]],\n",
    "                      node_batches = [output[7] for output in outputs[::2]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rotate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch3d.transforms import quaternion_apply, random_quaternions\n",
    "rot = random_quaternions(1, device=device)\n",
    "rot = rot/rot.norm(dim=-1, keepdim=True)\n",
    "trans = torch.randn(3,device=device)\n",
    "node_coord_rot = quaternion_apply(rot, node_coord)# + trans\n",
    "query_points_rot = quaternion_apply(rot, query_points)# + trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_feature_rot = scene_proc.colors\n",
    "node_emb_rot = node_enc(node_feature_rot)\n",
    "outputs_rot = down_block(node_feature=node_emb_rot,\n",
    "                         node_coord=node_coord_rot,\n",
    "                         batch=batch)\n",
    "\n",
    "field_val_rot = extractor(query_coord = query_points_rot, \n",
    "                      query_batch = torch.zeros_like(query_points[...,0]),\n",
    "                      node_features = [output[0] for output in outputs_rot[::2]],\n",
    "                      node_coords = [output[1] for output in outputs_rot[::2]],\n",
    "                      node_batches = [output[7] for output in outputs_rot[::2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Equivariance ratio: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hw/anaconda3/envs/diff_edf/lib/python3.8/site-packages/torch/nn/modules/module.py:1190: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at /opt/conda/conda-bld/pytorch_1666642991888/work/torch/csrc/jit/codegen/cuda/parser.cpp:3668.)\n",
      "  return forward_call(*input, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "irrep_transform = e3nn_script(TransformFeatureQuaternion(irreps = irreps_node_embedding, device=device))\n",
    "a = field_val_rot\n",
    "b = field_val\n",
    "isclose = torch.isclose(irrep_transform(b, rot)[0], a, atol=0.001, rtol=0.001)\n",
    "# print(isclose)\n",
    "print(f\"Equivariance ratio: {(isclose.sum() / len(a.view(-1))).item()}\") # Slight non-equivariance comes from FPS downsampling algorithm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diff_edf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16 | packaged by conda-forge | (default, Feb  1 2023, 16:01:55) \n[GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "79a0085b6cf04e1cff261ad12d41cff4e1530d9e68d1f8fc6bd159a2915452c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
