{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_JIT_USE_NNC_NOT_NVFUSER\"] = \"1\"\n",
    "from typing import List, Tuple, Optional, Union, Iterable\n",
    "import math\n",
    "\n",
    "from beartype import beartype\n",
    "import datetime\n",
    "import plotly.graph_objects as go\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Compose\n",
    "from e3nn import o3\n",
    "\n",
    "from edf_interface.data import PointCloud, SE3\n",
    "from diffusion_edf.gnn_data import FeaturedPoints\n",
    "from diffusion_edf import train_utils\n",
    "from diffusion_edf.trainer import DiffusionEdfTrainer\n",
    "from diffusion_edf.visualize import visualize_pose\n",
    "\n",
    "torch.set_printoptions(precision=4, sci_mode=False)\n",
    "device = 'cuda:0'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Low-resolution Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs_root_dir = 'configs/pick_lowres'\n",
    "train_configs_file = 'train_configs.yaml'\n",
    "task_configs_file = 'task_configs.yaml'\n",
    "\n",
    "lowres_trainer = DiffusionEdfTrainer(\n",
    "    configs_root_dir=configs_root_dir,\n",
    "    train_configs_file=train_configs_file,\n",
    "    task_configs_file=task_configs_file,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "lowres_trainer._init_dataloaders()\n",
    "lowres_model = lowres_trainer.get_model(\n",
    "    checkpoint_dir='example_runs/2023_06_01_21-36-31_Pick_LowRes/checkpoint/200.pt',\n",
    "    deterministic=False, \n",
    "    device = device\n",
    ").eval()\n",
    "\n",
    "lowres_trainer.warmup_score_model(\n",
    "    score_model=lowres_model, \n",
    "    n_warmups=10\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Super-resolution Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs_root_dir = 'configs/pick_highres'\n",
    "train_configs_file = 'train_configs.yaml'\n",
    "task_configs_file = 'task_configs.yaml'\n",
    "\n",
    "highres_trainer = DiffusionEdfTrainer(\n",
    "    configs_root_dir=configs_root_dir,\n",
    "    train_configs_file=train_configs_file,\n",
    "    task_configs_file=task_configs_file,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "highres_trainer._init_dataloaders()\n",
    "highres_model = highres_trainer.get_model(\n",
    "    checkpoint_dir='example_runs/2023_06_01_21-47-51_Pick_HiRes/checkpoint/200.pt',\n",
    "    deterministic=False, \n",
    "    device = device,\n",
    ").eval()\n",
    "\n",
    "highres_trainer.warmup_score_model(\n",
    "    score_model=highres_model, \n",
    "    n_warmups=10\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Input Data and Initial Pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## Load test data ######################\n",
    "\n",
    "# dataset = list(lowres_trainer.trainloader)\n",
    "dataset = list(lowres_trainer.testloader)\n",
    "demo_batch = dataset[3]\n",
    "B = len(demo_batch)\n",
    "assert B == 1, \"Batch training is not supported yet.\"\n",
    "\n",
    "scene_input, grasp_input, _ = train_utils.flatten_batch(demo_batch=demo_batch)\n",
    "scene_pcd = PointCloud(points=scene_input.x, colors=scene_input.f)\n",
    "grasp_pcd = PointCloud(points=grasp_input.x, colors=grasp_input.f)\n",
    "\n",
    "##################### Initial pose #####################\n",
    "\n",
    "#### Random pose ####\n",
    "# T0 = torch.cat([\n",
    "#     transforms.random_quaternions(1, device=device),\n",
    "#     torch.distributions.Uniform(scene_input.x[:].min(dim=0).values, scene_input.x[:].max(dim=0).values).sample(sample_shape=(1,))\n",
    "# ], dim=-1)\n",
    "\n",
    "#### Pose 1 ####\n",
    "# T0 = torch.cat([\n",
    "#     transforms.random_quaternions(1, device=device),\n",
    "#     torch.tensor([[math.sqrt(0.5), -math.sqrt(0.5), 0.0, 0.]], device=device),\n",
    "# ], dim=-1)\n",
    "\n",
    "#### Pose 2 ####\n",
    "T0 = torch.cat([\n",
    "    torch.tensor([[1., 0., 0.0, 0.]], device=device),\n",
    "    torch.tensor([[-30., -30., 30.]], device=device)\n",
    "], dim=-1)\n",
    "\n",
    "################## Visualize Diffused Pose #################\n",
    "\n",
    "# diffused_pose_pcd = PointCloud.merge(\n",
    "#     scene_pcd,\n",
    "#     grasp_pcd.transformed(SE3(T0))[0],\n",
    "# )\n",
    "# diffused_pose_pcd.show(point_size=2., width=600, height=600)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Low-resolution Diffusion Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### Feature extraction #####################\n",
    "with torch.no_grad():\n",
    "    scene_out_multiscale: List[FeaturedPoints] = lowres_model.get_key_pcd_multiscale(scene_input)\n",
    "    grasp_out: FeaturedPoints = lowres_model.get_query_pcd(grasp_input)\n",
    "\n",
    "\n",
    "################ Visualize Scene Attention Map #####################\n",
    "\n",
    "# scene_attn_pcd = PointCloud(points=scene_out_multiscale[0].x.detach().cpu(), \n",
    "#                             colors=scene_out_multiscale[0].w.detach().cpu(),\n",
    "#                             cmap='magma')\n",
    "# scene_attn_pcd.show(point_size=6., width=600, height=600)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### Sample #####################\n",
    "with torch.no_grad():\n",
    "    Ts_lowres = lowres_model.sample(T_seed=T0.clone().detach(),\n",
    "                                    scene_pcd_multiscale=scene_out_multiscale,\n",
    "                                    grasp_pcd=grasp_out,\n",
    "                                    diffusion_schedules=lowres_trainer.diffusion_schedules,\n",
    "                                    N_steps=[500, 500],\n",
    "                                    timesteps=[0.02, 0.02],\n",
    "                                    temperature=1.)\n",
    "    T_lowres = Ts_lowres[-1:,:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Super-resolution Diffusion Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    scene_out_multiscale: List[FeaturedPoints] = highres_model.get_key_pcd_multiscale(scene_input)\n",
    "    grasp_out: FeaturedPoints = highres_model.get_query_pcd(grasp_input)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    Ts_highres = highres_model.sample(T_seed=T_lowres.clone().detach(),\n",
    "                                      scene_pcd_multiscale=scene_out_multiscale,\n",
    "                                      grasp_pcd=grasp_out,\n",
    "                                      diffusion_schedules=highres_trainer.diffusion_schedules,\n",
    "                                      N_steps=[500, 1000],\n",
    "                                      timesteps=[0.02, 0.05],\n",
    "                                      temperature=1.)\n",
    "    T_highres = Ts_highres[-1:,:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     scene_out_multiscale: List[FeaturedPoints] = highres_model.get_key_pcd_multiscale(scene_input)\n",
    "#     grasp_out: FeaturedPoints = highres_model.get_query_pcd(grasp_input)\n",
    "    \n",
    "#     Ts_optim = highres_model.sample(T_seed=T_highres.clone().detach(),\n",
    "#                                     scene_pcd_multiscale=scene_out_multiscale,\n",
    "#                                     grasp_pcd=grasp_out,\n",
    "#                                     diffusion_schedules=[[0.005, 0.003]],\n",
    "#                                     N_steps=[1000],\n",
    "#                                     timesteps=[0.05],\n",
    "#                                     temperature=1.,\n",
    "#                                     ang_noise_mult=0.,\n",
    "#                                     lin_noise_mult=0.,)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import importlib\n",
    "# import diffusion_edf.visualize\n",
    "# importlib.reload(diffusion_edf.visualize)\n",
    "# visualize_pose = diffusion_edf.visualize.visualize_pose\n",
    "\n",
    "Ts = torch.cat([Ts_lowres, Ts_highres], dim=0).float()\n",
    "# Ts = torch.cat([Ts_lowres, Ts_highres, Ts_optim], dim=0).float()\n",
    "Ts_visualize = torch.cat([Ts[::10], Ts[-1:]], dim=0)\n",
    "fig_grasp, fig_sample = visualize_pose(scene_pcd, grasp_pcd, poses=SE3(Ts_visualize), \n",
    "                                       point_size=3.0, width=800, height=800,\n",
    "                                       ranges=torch.tensor([[-40., 40.], [-40., 40.], [0., 40.]]))\n",
    "fig_sample.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diff_edf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "79a0085b6cf04e1cff261ad12d41cff4e1530d9e68d1f8fc6bd159a2915452c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
