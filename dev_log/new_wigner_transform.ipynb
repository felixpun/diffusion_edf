{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Dict, Union, Optional, Tuple\n",
    "from beartype import beartype\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "from e3nn import o3\n",
    "# from e3nn.o3._wigner import _Jd\n",
    "# _Jd: Tuple[torch.Tensor] = tuple(J.detach().clone().to(dtype=torch.float32) for J in _Jd)\n",
    "from e3nn.math._linalg import direct_sum\n",
    "from e3nn.util.jit import compile_mode\n",
    "from diffusion_edf.transforms import matrix_to_euler_angles, quaternion_to_matrix, standardize_quaternion, random_quaternions\n",
    "from diffusion_edf import w3j\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusion_edf.wigner import quat_to_angle_fast, transform_feature_slice_nonscalar\n",
    "\n",
    "class SliceAndTransform(torch.nn.Module):\n",
    "    mul: torch.jit.Final[int]\n",
    "    l: torch.jit.Final[int]\n",
    "    dim: torch.jit.Final[int]\n",
    "    start: torch.jit.Final[int]\n",
    "    len: torch.jit.Final[int]\n",
    "    \n",
    "    def __init__(self, mul:int, l: int, start: int, end: int, allow_zero_len: bool = False):\n",
    "        super().__init__()\n",
    "        self.mul = mul\n",
    "        self.l = l\n",
    "        self.dim = 2*self.l+1\n",
    "        self.register_buffer(\"J\", w3j._Jd[l].detach().clone())\n",
    "        \n",
    "        if allow_zero_len:\n",
    "            assert end >= start, f\"end ({end}) < start ({start})\"\n",
    "        else:\n",
    "            assert end > start, f\"end ({end}) =< start ({start})\"\n",
    "        self.start = start\n",
    "        self.len = end - start\n",
    "        \n",
    "    def forward(self, feature: torch.Tensor, alpha: torch.Tensor, beta: torch.Tensor, gamma: torch.Tensor) -> torch.Tensor:\n",
    "        sliced = torch.narrow(feature, dim=-1, start=self.start, length=self.len)\n",
    "        assert sliced.shape[-1] == self.len, f\"{sliced.shape[-1]} != {self.len}\"\n",
    "        if self.l == 0:\n",
    "            return sliced.expand(len(alpha), len(sliced), self.len)\n",
    "        else:\n",
    "            return transform_feature_slice_nonscalar(feature=sliced, alpha=alpha, beta=beta, gamma=gamma, l=self.l, J=self.J)\n",
    "\n",
    "class TransformFeatureQuaternion(torch.nn.Module):\n",
    "    dim: torch.jit.Final[int]\n",
    "    lmax: torch.jit.Final[int]\n",
    "    # __constants__ = ['Js']\n",
    "    \n",
    "    def __init__(self, irreps: Union[str, o3.Irreps]):\n",
    "        super().__init__()\n",
    "\n",
    "        irreps = o3.Irreps(irreps)\n",
    "        for n, (l,p) in irreps:\n",
    "            if p != 1:\n",
    "                raise NotImplementedError(f\"E3 equivariance is not implemented! (input_irreps: {irreps})\")\n",
    "        self.dim = irreps.dim\n",
    "        self.lmax = irreps.lmax\n",
    "        \n",
    "        self.transforms = torch.nn.ModuleList()\n",
    "        for (mul, l), (start, end) in zip(\n",
    "            tuple((mul, ir.l) for mul, ir in irreps), \n",
    "            tuple((slice_.start, slice_.stop) for slice_ in irreps.slices())\n",
    "        ):\n",
    "            self.transforms.append(\n",
    "                SliceAndTransform(mul=mul, l=l, start=start, end=end, allow_zero_len=False)\n",
    "            )\n",
    "        \n",
    "        \n",
    "    def forward(self, feature: torch.Tensor, q: torch.Tensor) -> torch.Tensor : # (N_Q, N_D) x (N_T, 4) -> (N_T, N_Q, N_D)\n",
    "        assert q.ndim == 2 and q.shape[-1] == 4, f\"{q.shape}\" # (nT, 4)\n",
    "        assert feature.ndim == 2 and feature.shape[-1] == self.dim, f\"{feature.shape}\" # (nQ, D)\n",
    "\n",
    "        # --------------------------------------------------- #\n",
    "        # Return Identity if spin-0 only\n",
    "        # --------------------------------------------------- #\n",
    "        if self.lmax == 0:\n",
    "            return feature.expand(len(q), -1, -1)\n",
    "        \n",
    "        # --------------------------------------------------- #\n",
    "        # Quaternion to Euler angles\n",
    "        # --------------------------------------------------- #\n",
    "        q = standardize_quaternion(q / torch.norm(q, dim=-1, keepdim=True))\n",
    "        angle = quat_to_angle_fast(q)\n",
    "        alpha, beta, gamma = angle[0], angle[1], angle[2]\n",
    "        \n",
    "        # --------------------------------------------------- #\n",
    "        # Quaternion to Euler angles\n",
    "        # --------------------------------------------------- #\n",
    "        feature_transformed = []\n",
    "        for transform in self.transforms:\n",
    "            feature_transformed.append(\n",
    "                transform(feature=feature, alpha=alpha, beta=beta, gamma=gamma)\n",
    "            )\n",
    "        \n",
    "        return torch.cat(feature_transformed, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusion_edf.wigner import transform_feature_quat_\n",
    "class OldTransformFeatureQuaternion(torch.nn.Module):\n",
    "    def __init__(self, irreps: o3.Irreps):\n",
    "        super().__init__()\n",
    "        self.ls = tuple([ir.l for mul, ir in irreps])\n",
    "        self.slices = tuple([(slice_.start, slice_.stop) for slice_ in irreps.slices()])\n",
    "        self.Js = tuple(w3j._Jd[l] for l in self.ls)\n",
    "        self.dim: int = o3.Irreps(irreps).dim\n",
    "\n",
    "        for n, (l,p) in o3.Irreps(irreps):\n",
    "            if p != 1:\n",
    "                raise NotImplementedError(f\"E3 equivariance is not implemented! (input_irreps: {o3.Irreps(irreps)})\")\n",
    "        self.lmax: int = o3.Irreps(irreps).lmax\n",
    "        \n",
    "    @torch.jit.ignore()\n",
    "    def to(self, *args, **kwargs):\n",
    "        self.Js = tuple(w3j._Jd[l].to(*args, **kwargs) for l in self.ls)\n",
    "        for module in self.children():\n",
    "            if isinstance(module, torch.nn.Module):\n",
    "                module.to(*args, **kwargs)\n",
    "        return super().to(*args, **kwargs)\n",
    "\n",
    "    def forward(self, feature: torch.Tensor, q: torch.Tensor) -> torch.Tensor : # (N_Q, N_D) x (N_T, 4) -> (N_T, N_Q, N_D)\n",
    "        assert q.ndim == 2 and q.shape[-1] == 4, f\"{q.shape}\" # (nT, 4)\n",
    "        assert feature.ndim == 2 and feature.shape[-1] == self.dim, f\"{feature.shape}\" # (nQ, D)\n",
    "\n",
    "        if self.lmax == 0:\n",
    "            return feature.expand(len(q), -1, -1)\n",
    "        \n",
    "        feature_slices = []\n",
    "        for slice_ in self.slices:\n",
    "            feature_slices.append(feature[..., slice_[0]:slice_[1]])\n",
    "\n",
    "        return transform_feature_quat_(ls=self.ls, feature_slices=feature_slices, Js=self.Js, q=q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "irreps = o3.Irreps(\"10x0e+12x1e+3x2e\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = TransformFeatureQuaternion(irreps=irreps)\n",
    "trans = torch.jit.script(trans)\n",
    "trans = trans.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:01<00:00, 47.65it/s]\n"
     ]
    }
   ],
   "source": [
    "feature = irreps.randn(100,-1, device=device)\n",
    "q=random_quaternions(100, device=device)\n",
    "for _ in tqdm(range(50)):\n",
    "    trans(feature, q) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_trans = OldTransformFeatureQuaternion(irreps=irreps)\n",
    "old_trans = old_trans.to(device)\n",
    "old_trans = torch.jit.script(old_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]/home/hw/anaconda3/envs/diff_edf/lib/python3.8/site-packages/torch/nn/modules/module.py:1194: UserWarning: operator() sees varying value in profiling, ignoring and this should be handled by GUARD logic (Triggered internally at ../torch/csrc/jit/codegen/cuda/parser.cpp:3668.)\n",
      "  return forward_call(*input, **kwargs)\n",
      "/home/hw/anaconda3/envs/diff_edf/lib/python3.8/site-packages/torch/nn/modules/module.py:1194: UserWarning: operator() profile_node %304 : int[] = prim::profile_ivalue[profile_failed=\"varying profile values\"](%302)\n",
      " does not have profile information (Triggered internally at ../torch/csrc/jit/codegen/cuda/graph_fuser.cpp:105.)\n",
      "  return forward_call(*input, **kwargs)\n",
      "100%|██████████| 50/50 [00:00<00:00, 305.62it/s]\n"
     ]
    }
   ],
   "source": [
    "feature = irreps.randn(100,-1, device=device)\n",
    "q=random_quaternions(100, device=device)\n",
    "for _ in tqdm(range(50)):\n",
    "    old_trans(feature, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "irreps = o3.Irreps(\"10x0e+12x1e+3x2e\")\n",
    "feature = irreps.randn(100,-1, device=device)\n",
    "q=random_quaternions(100, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:03<00:00, 3016.57it/s]\n"
     ]
    }
   ],
   "source": [
    "for _ in tqdm(range(10000)):\n",
    "    trans(feature, q) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:03<00:00, 2945.21it/s]\n"
     ]
    }
   ],
   "source": [
    "for _ in tqdm(range(10000)):\n",
    "    old_trans(feature, q) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New module is almost identical in speed but does not generate warning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diff_edf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
