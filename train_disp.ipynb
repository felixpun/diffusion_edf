{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Tuple, Optional, Union, Iterable\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Compose\n",
    "from pytorch3d.transforms import quaternion_apply, random_quaternions, quaternion_multiply, quaternion_invert, axis_angle_to_quaternion, se3_exp_map, se3_log_map, quaternion_to_matrix, matrix_to_quaternion\n",
    "\n",
    "from e3nn import o3\n",
    "\n",
    "from diffusion_edf.embedding import NodeEmbeddingNetwork\n",
    "from diffusion_edf.data import SE3, PointCloud, TargetPoseDemo, DemoSequence, DemoSeqDataset, load_demos, save_demos\n",
    "from diffusion_edf.preprocess import Rescale, NormalizeColor, Downsample, PointJitter, ColorJitter\n",
    "from diffusion_edf.wigner import TransformFeatureQuaternion\n",
    "from diffusion_edf.score_model import ScoreModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_len = 0.01\n",
    "scene_voxel_size = 0.01\n",
    "grasp_voxel_size = 0.01\n",
    "\n",
    "scene_voxel_size = scene_voxel_size / unit_len\n",
    "grasp_voxel_size = grasp_voxel_size / unit_len\n",
    "\n",
    "\n",
    "rescale_fn = Rescale(rescale_factor=1/unit_len)\n",
    "recover_scale_fn = Rescale(rescale_factor=unit_len)\n",
    "normalize_color_fn = NormalizeColor(color_mean = torch.tensor([0.5, 0.5, 0.5]), color_std = torch.tensor([0.5, 0.5, 0.5]))\n",
    "recover_color_fn = NormalizeColor(color_mean = -normalize_color_fn.color_mean / normalize_color_fn.color_std, color_std = 1 / normalize_color_fn.color_std)\n",
    "\n",
    "\n",
    "scene_proc_fn = Compose([rescale_fn,\n",
    "                         Downsample(voxel_size=scene_voxel_size, coord_reduction=\"average\"),\n",
    "                         normalize_color_fn])\n",
    "scene_unproc_fn = Compose([recover_color_fn, recover_scale_fn])\n",
    "grasp_proc_fn = Compose([rescale_fn,\n",
    "                         Downsample(voxel_size=grasp_voxel_size, coord_reduction=\"average\"),\n",
    "                         normalize_color_fn])\n",
    "grasp_unproc_fn = Compose([recover_color_fn, recover_scale_fn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "device = 'cuda:0'\n",
    "compile = False\n",
    "eval = True\n",
    "\n",
    "irreps_input = o3.Irreps('3x0e')\n",
    "irreps_node_embedding = o3.Irreps('32x0e+16x1e+8x2e') #o3.Irreps('128x0e+64x1e+32x2e')\n",
    "irreps_sh = o3.Irreps('1x0e+1x1e+1x2e')\n",
    "fc_neurons = [128, 64, 64]\n",
    "num_heads = 4\n",
    "alpha_drop = 0.2\n",
    "proj_drop = 0.0\n",
    "drop_path_rate = 0.0\n",
    "irreps_mlp_mid = 2\n",
    "n_scales = 4\n",
    "pool_ratio = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hw/anaconda3/envs/diff_edf/lib/python3.8/site-packages/torch/jit/_check.py:181: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\"The TorchScript type system doesn't support \"\n"
     ]
    }
   ],
   "source": [
    "score_model = ScoreModel(irreps_input = irreps_input,\n",
    "                         irreps_emb_init = irreps_node_embedding,\n",
    "                         irreps_sh = irreps_sh,\n",
    "                         fc_neurons_init = [32, 16, 16],\n",
    "                         num_heads = 4,\n",
    "                         n_scales = 4,\n",
    "                         pool_ratio = 0.5,\n",
    "                         dim_mult = [1, 2, 3, 4],\n",
    "                         n_layers = 2,\n",
    "                         gnn_radius = 2.0,\n",
    "                         cutoff_radius = 3.0,\n",
    "                         weight_feature_dim = 20,\n",
    "                         query_downsample_ratio = 0.3,\n",
    "                         device=device,\n",
    "                         deterministic = True,\n",
    "                         compile_head = compile)\n",
    "\n",
    "score_model = score_model.to(device)\n",
    "if eval:\n",
    "    score_model = score_model.eval()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(list(score_model.parameters()), lr=1e-4, betas=(0.9, 0.98), eps=1e-09, weight_decay=1e-4, amsgrad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.MSELoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = DemoSeqDataset(dataset_dir=\"demo/test_demo\", annotation_file=\"data.yaml\", device=device)\n",
    "train_dataloader = DataLoader(trainset, shuffle=True, collate_fn=lambda x:x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad(set_to_none=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.4813)\n",
      "tensor(4.5744)\n",
      "tensor(4.1819)\n",
      "tensor(4.9649)\n",
      "tensor(4.4160)\n",
      "tensor(4.4867)\n",
      "tensor(4.8789)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cannot reshape tensor of 0 elements into shape [0, 4, -1] because the unspecified dimension size -1 can be any value and is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 42\u001b[0m\n\u001b[1;32m     39\u001b[0m query_coord \u001b[39m=\u001b[39m grasp_proc\u001b[39m.\u001b[39mpoints\n\u001b[1;32m     40\u001b[0m query_batch \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(\u001b[39mlen\u001b[39m(query_coord), device\u001b[39m=\u001b[39mdevice, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong)\n\u001b[0;32m---> 42\u001b[0m lin_vel, ang_vel \u001b[39m=\u001b[39m score_model(T\u001b[39m=\u001b[39;49mT,\n\u001b[1;32m     43\u001b[0m                                key_feature\u001b[39m=\u001b[39;49mkey_feature, key_coord\u001b[39m=\u001b[39;49mkey_coord, key_batch\u001b[39m=\u001b[39;49mkey_batch,\n\u001b[1;32m     44\u001b[0m                                query_feature\u001b[39m=\u001b[39;49mquery_feature, query_coord\u001b[39m=\u001b[39;49mquery_coord, query_batch\u001b[39m=\u001b[39;49mquery_batch)\n\u001b[1;32m     46\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(torch\u001b[39m.\u001b[39mcat([ang_vel, lin_vel], dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m), torch\u001b[39m.\u001b[39mcat([disp_q, disp_x], dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[1;32m     47\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/envs/diff_edf/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Codes/Research/diffusion_edf/diffusion_edf/score_model.py:221\u001b[0m, in \u001b[0;36mScoreModel.forward\u001b[0;34m(self, T, key_feature, key_coord, key_batch, query_feature, query_coord, query_batch)\u001b[0m\n\u001b[1;32m    218\u001b[0m query \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_query(node_feature\u001b[39m=\u001b[39mquery_feature, node_coord\u001b[39m=\u001b[39mquery_coord, batch\u001b[39m=\u001b[39mquery_batch)\n\u001b[1;32m    219\u001b[0m key_gnn_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_gnn_features(node_feature\u001b[39m=\u001b[39mkey_feature, node_coord\u001b[39m=\u001b[39mkey_coord, batch\u001b[39m=\u001b[39mkey_batch)\n\u001b[0;32m--> 221\u001b[0m lin_vel, ang_vel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_score(T\u001b[39m=\u001b[39;49mT, query\u001b[39m=\u001b[39;49mquery, key_gnn_features\u001b[39m=\u001b[39;49mkey_gnn_features)\n\u001b[1;32m    223\u001b[0m \u001b[39mreturn\u001b[39;00m lin_vel, ang_vel\n",
      "File \u001b[0;32m~/Codes/Research/diffusion_edf/diffusion_edf/score_model.py:213\u001b[0m, in \u001b[0;36mScoreModel.get_score\u001b[0;34m(self, T, query, key_gnn_features)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_score\u001b[39m(\u001b[39mself\u001b[39m, T: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m    211\u001b[0m               query: Tuple[torch\u001b[39m.\u001b[39mTensor, torch\u001b[39m.\u001b[39mTensor, torch\u001b[39m.\u001b[39mTensor, torch\u001b[39m.\u001b[39mTensor], \n\u001b[1;32m    212\u001b[0m               key_gnn_features: List[Tuple[torch\u001b[39m.\u001b[39mTensor, torch\u001b[39m.\u001b[39mTensor, torch\u001b[39m.\u001b[39mTensor]]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor, torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m--> 213\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkey_head\u001b[39m.\u001b[39;49mget_score(T\u001b[39m=\u001b[39;49mT, query\u001b[39m=\u001b[39;49mquery, key_gnn_features\u001b[39m=\u001b[39;49mkey_gnn_features)\n",
      "File \u001b[0;32m~/Codes/Research/diffusion_edf/diffusion_edf/score_model.py:100\u001b[0m, in \u001b[0;36mScoreModelHead.get_score\u001b[0;34m(self, T, query, key_gnn_features)\u001b[0m\n\u001b[1;32m     97\u001b[0m query_feature_transformed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform_key_irreps(query_feature, q)\u001b[39m.\u001b[39mcontiguous()\u001b[39m.\u001b[39mview(N_T\u001b[39m*\u001b[39mN_Q, N_D) \u001b[39m# (Nq, D) x (Nt, 4) -> (Nt * Nq, D)\u001b[39;00m\n\u001b[1;32m     98\u001b[0m batch_repeat \u001b[39m=\u001b[39m batch\u001b[39m.\u001b[39mexpand(N_T,N_Q)\u001b[39m.\u001b[39mcontiguous()\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m# (N_T*N_Q,)\u001b[39;00m\n\u001b[0;32m--> 100\u001b[0m key_feature: torch\u001b[39m.\u001b[39mTensor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_key(query_points\u001b[39m=\u001b[39;49mquery_coord_transformed\u001b[39m.\u001b[39;49mview(N_T \u001b[39m*\u001b[39;49m N_Q ,\u001b[39m3\u001b[39;49m), query_batch\u001b[39m=\u001b[39;49mbatch_repeat, gnn_features\u001b[39m=\u001b[39;49mkey_gnn_features)\u001b[39m# (N_T * N_Q, N_D)\u001b[39;00m\n\u001b[1;32m    102\u001b[0m lin_vel: torch\u001b[39m.\u001b[39mTensor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlin_vel_tp(query_feature_transformed, key_feature, edge_scalars\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, batch\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m) \u001b[39m# batch does nothing unless you use batchnorm\u001b[39;00m\n\u001b[1;32m    103\u001b[0m ang_spin: torch\u001b[39m.\u001b[39mTensor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mang_vel_tp(query_feature_transformed, key_feature, edge_scalars\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, batch\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m) \u001b[39m# batch does nothing unless you use batchnorm\u001b[39;00m\n",
      "File \u001b[0;32m~/Codes/Research/diffusion_edf/diffusion_edf/score_model.py:74\u001b[0m, in \u001b[0;36mScoreModelHead._get_key\u001b[0;34m(self, query_points, query_batch, gnn_features)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[39m@torch\u001b[39m\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mexport\n\u001b[1;32m     70\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_key\u001b[39m(\u001b[39mself\u001b[39m, query_points: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m     71\u001b[0m              query_batch: torch\u001b[39m.\u001b[39mTensor,\n\u001b[1;32m     72\u001b[0m              gnn_features: List[Tuple[torch\u001b[39m.\u001b[39mTensor, torch\u001b[39m.\u001b[39mTensor, torch\u001b[39m.\u001b[39mTensor]]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m---> 74\u001b[0m     field_val \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkey_extractor(query_coord \u001b[39m=\u001b[39;49m query_points, \n\u001b[1;32m     75\u001b[0m                                    query_batch \u001b[39m=\u001b[39;49m query_batch,\n\u001b[1;32m     76\u001b[0m                                    node_features \u001b[39m=\u001b[39;49m [output[\u001b[39m0\u001b[39;49m] \u001b[39mfor\u001b[39;49;00m output \u001b[39min\u001b[39;49;00m gnn_features],\n\u001b[1;32m     77\u001b[0m                                    node_coords \u001b[39m=\u001b[39;49m [output[\u001b[39m1\u001b[39;49m] \u001b[39mfor\u001b[39;49;00m output \u001b[39min\u001b[39;49;00m gnn_features],\n\u001b[1;32m     78\u001b[0m                                    node_batches \u001b[39m=\u001b[39;49m [output[\u001b[39m2\u001b[39;49m] \u001b[39mfor\u001b[39;49;00m output \u001b[39min\u001b[39;49;00m gnn_features])\n\u001b[1;32m     80\u001b[0m     \u001b[39mreturn\u001b[39;00m field_val\n",
      "File \u001b[0;32m~/anaconda3/envs/diff_edf/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Codes/Research/diffusion_edf/diffusion_edf/block.py:290\u001b[0m, in \u001b[0;36mEdfExtractor.forward\u001b[0;34m(self, query_coord, query_batch, node_features, node_coords, node_batches)\u001b[0m\n\u001b[1;32m    286\u001b[0m     edge_length \u001b[39m=\u001b[39m edge_vec\u001b[39m.\u001b[39mnorm(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, p\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m    287\u001b[0m     edge_scalar \u001b[39m=\u001b[39m radial(edge_length)\n\u001b[1;32m    289\u001b[0m     node_feature_dst \u001b[39m=\u001b[39m node_feature_dst \\\n\u001b[0;32m--> 290\u001b[0m                        \u001b[39m+\u001b[39m layers(node_input_src \u001b[39m=\u001b[39;49m node_features[n],\n\u001b[1;32m    291\u001b[0m                                 node_input_dst \u001b[39m=\u001b[39;49m (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mzero_features\u001b[39m.\u001b[39;49mdetach())\u001b[39m.\u001b[39;49mexpand(\u001b[39mlen\u001b[39;49m(query_coord), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49memb_dim),\n\u001b[1;32m    292\u001b[0m                                 batch_dst \u001b[39m=\u001b[39;49m query_batch,\n\u001b[1;32m    293\u001b[0m                                 edge_src \u001b[39m=\u001b[39;49m edge_src,\n\u001b[1;32m    294\u001b[0m                                 edge_dst \u001b[39m=\u001b[39;49m edge_dst,\n\u001b[1;32m    295\u001b[0m                                 edge_attr \u001b[39m=\u001b[39;49m edge_attr,\n\u001b[1;32m    296\u001b[0m                                 edge_scalars \u001b[39m=\u001b[39;49m edge_scalar)\n\u001b[1;32m    298\u001b[0m \u001b[39m# for n in range(self.n_layers - 1):\u001b[39;00m\n\u001b[1;32m    299\u001b[0m \u001b[39m#     _, _, edge_src, edge_dst, degree, _ = self.post_connect[n](node_coord_src = query_coord, \u001b[39;00m\n\u001b[1;32m    300\u001b[0m \u001b[39m#                                                                node_feature_src = node_feature_dst, \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[39m#                                            edge_attr = edge_attr,\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[39m#                                            edge_scalars = edge_scalar)\u001b[39;00m\n\u001b[1;32m    315\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproj(node_feature_dst)\n",
      "File \u001b[0;32m~/anaconda3/envs/diff_edf/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Codes/Research/diffusion_edf/diffusion_edf/block.py:145\u001b[0m, in \u001b[0;36mEquiformerBlock.forward\u001b[0;34m(self, node_input_src, node_input_dst, batch_dst, edge_src, edge_dst, edge_attr, edge_scalars)\u001b[0m\n\u001b[1;32m    141\u001b[0m message_dst: torch\u001b[39m.\u001b[39mTensor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear_dst(node_input_dst)\n\u001b[1;32m    143\u001b[0m message: torch\u001b[39m.\u001b[39mTensor \u001b[39m=\u001b[39m message_src[edge_src] \u001b[39m+\u001b[39m message_dst[edge_dst]\n\u001b[0;32m--> 145\u001b[0m node_features: torch\u001b[39m.\u001b[39mTensor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mga(message\u001b[39m=\u001b[39;49mmessage, \n\u001b[1;32m    146\u001b[0m                                       edge_dst\u001b[39m=\u001b[39;49medge_dst, \n\u001b[1;32m    147\u001b[0m                                       edge_attr\u001b[39m=\u001b[39;49medge_attr, \n\u001b[1;32m    148\u001b[0m                                       edge_scalars\u001b[39m=\u001b[39;49medge_scalars,\n\u001b[1;32m    149\u001b[0m                                       n_nodes_dst \u001b[39m=\u001b[39;49m \u001b[39mlen\u001b[39;49m(node_input_dst))\n\u001b[1;32m    151\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdrop_path \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    152\u001b[0m     node_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdrop_path(node_features, batch_dst)\n",
      "File \u001b[0;32m~/anaconda3/envs/diff_edf/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Codes/Research/diffusion_edf/diffusion_edf/graph_attention.py:90\u001b[0m, in \u001b[0;36mGraphAttentionMLP.forward\u001b[0;34m(self, message, edge_dst, edge_attr, edge_scalars, n_nodes_dst)\u001b[0m\n\u001b[1;32m     88\u001b[0m message: torch\u001b[39m.\u001b[39mTensor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msep_act\u001b[39m.\u001b[39mdtp(message, edge_attr, weight)\n\u001b[1;32m     89\u001b[0m alpha: torch\u001b[39m.\u001b[39mTensor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msep_alpha(message)                        \u001b[39m# f_ij^(L=0) part  ||  Linear: irreps_in -> 'mul_alpha x 0e'\u001b[39;00m\n\u001b[0;32m---> 90\u001b[0m alpha: torch\u001b[39m.\u001b[39mTensor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvec2heads_alpha(alpha)                    \u001b[39m# reshape (N, Heads*head_dim) -> (N, Heads, head_dim)\u001b[39;00m\n\u001b[1;32m     91\u001b[0m value: torch\u001b[39m.\u001b[39mTensor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msep_act\u001b[39m.\u001b[39mlin(message)                      \u001b[39m# f_ij^(L>=0) part (before activation)\u001b[39;00m\n\u001b[1;32m     92\u001b[0m value: torch\u001b[39m.\u001b[39mTensor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msep_act\u001b[39m.\u001b[39mgate(value)                       \u001b[39m# f_ij^(L>=0) part (after activation)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/diff_edf/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Codes/Research/diffusion_edf/diffusion_edf/equiformer/graph_attention_transformer.py:165\u001b[0m, in \u001b[0;36mVec2AttnHeads.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[39mfor\u001b[39;00m start_idx, end_idx \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmid_in_indices:\n\u001b[1;32m    164\u001b[0m     temp \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mnarrow(\u001b[39m1\u001b[39m, start_idx, end_idx \u001b[39m-\u001b[39m start_idx)\n\u001b[0;32m--> 165\u001b[0m     temp \u001b[39m=\u001b[39m temp\u001b[39m.\u001b[39;49mreshape(N, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_heads, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m    166\u001b[0m     out\u001b[39m.\u001b[39mappend(temp)\n\u001b[1;32m    167\u001b[0m out \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(out, dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cannot reshape tensor of 0 elements into shape [0, 4, -1] because the unspecified dimension size -1 can be any value and is ambiguous"
     ]
    }
   ],
   "source": [
    "max_epochs = 10\n",
    "N_samples = 10\n",
    "\n",
    "iter = 0\n",
    "for epoch in range(1, max_epochs+1):\n",
    "    for train_batch in train_dataloader:\n",
    "        iter += 1\n",
    "        assert len(train_batch) == 1, \"Batch training is not supported yet.\"\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        demo_seq: DemoSequence = train_batch[0]\n",
    "        demo: TargetPoseDemo = demo_seq[1]\n",
    "\n",
    "        scene_raw: PointCloud = demo.scene_pc\n",
    "        grasp_raw: PointCloud = demo.grasp_pc\n",
    "        target_poses: SE3 = demo.target_poses\n",
    "\n",
    "        scene_proc = scene_proc_fn(scene_raw).to(device)\n",
    "        grasp_proc = grasp_proc_fn(grasp_raw).to(device)\n",
    "        target_poses = rescale_fn(target_poses).to(device)\n",
    "\n",
    "        T_target = target_poses.poses\n",
    "\n",
    "        disp_q = torch.randn(N_samples, 3, device=device) * 0.5\n",
    "        disp_x = torch.randn(N_samples, 3, device=device) * 3.0\n",
    "        disp_T = se3_exp_map(torch.cat([disp_x, disp_q], dim=-1))\n",
    "        disp_T = torch.cat([matrix_to_quaternion(disp_T[...,:3,:3]), disp_T[...,-1,:3]], dim=-1)\n",
    "\n",
    "\n",
    "        T = torch.cat([quaternion_multiply(T_target[..., :4], disp_T[..., :4]), \n",
    "                    quaternion_apply(T_target[..., :4], disp_T[..., 4:]) + T_target[..., 4:]], dim=-1)\n",
    "\n",
    "\n",
    "        key_feature = scene_proc.colors\n",
    "        key_coord = scene_proc.points\n",
    "        key_batch = torch.zeros(len(key_coord), device=device, dtype=torch.long)\n",
    "        query_feature = grasp_proc.colors\n",
    "        query_coord = grasp_proc.points\n",
    "        query_batch = torch.zeros(len(query_coord), device=device, dtype=torch.long)\n",
    "\n",
    "        lin_vel, ang_vel = score_model(T=T,\n",
    "                                       key_feature=key_feature, key_coord=key_coord, key_batch=key_batch,\n",
    "                                       query_feature=query_feature, query_coord=query_coord, query_batch=query_batch)\n",
    "        \n",
    "        loss = loss_fn(torch.cat([ang_vel, lin_vel], dim=-1), torch.cat([disp_q, disp_x], dim=-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_list.append(loss.item())\n",
    "    print(torch.tensor(loss_list[-len(trainset):]).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "# for obj in gc.get_objects():\n",
    "#     try:\n",
    "#         if torch.is_tensor(obj) or (hasattr(obj, 'data') and torch.is_tensor(obj.data)):\n",
    "#             print(type(obj), obj.size())\n",
    "#     except:\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diff_edf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "79a0085b6cf04e1cff261ad12d41cff4e1530d9e68d1f8fc6bd159a2915452c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
