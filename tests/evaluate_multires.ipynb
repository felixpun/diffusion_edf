{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_JIT_USE_NNC_NOT_NVFUSER\"] = \"1\"\n",
    "from typing import List, Tuple, Optional, Union, Iterable\n",
    "import math\n",
    "\n",
    "from beartype import beartype\n",
    "import datetime\n",
    "import plotly.graph_objects as go\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Compose\n",
    "from e3nn import o3\n",
    "\n",
    "from diffusion_edf.data import DemoSeqDataset, DemoSequence, TargetPoseDemo, PointCloud, SE3\n",
    "from diffusion_edf.gnn_data import FeaturedPoints, merge_featured_points, GraphEdge, flatten_featured_points, set_featured_points_attribute, _featured_points_repr, detach_featured_points\n",
    "from diffusion_edf import train_utils\n",
    "from diffusion_edf import transforms\n",
    "from diffusion_edf.utils import sample_reference_points\n",
    "from diffusion_edf.dist import diffuse_isotropic_se3, adjoint_inv_tr_isotropic_se3_score, diffuse_isotropic_se3_batched\n",
    "from diffusion_edf.point_attentive_score_model import PointAttentiveScoreModel\n",
    "from diffusion_edf.trainer import DiffusionEdfTrainer\n",
    "from diffusion_edf.visualize import visualize_pose\n",
    "\n",
    "torch.set_printoptions(precision=4, sci_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs_root_dir = 'configs/pick_lowres'\n",
    "train_configs_file = 'train_configs.yaml'\n",
    "task_configs_file = 'task_configs.yaml'\n",
    "lowres_trainer = DiffusionEdfTrainer(configs_root_dir=configs_root_dir,\n",
    "                              train_configs_file=train_configs_file,\n",
    "                              task_configs_file=task_configs_file)\n",
    "configs_root_dir = 'configs/pick_highres'\n",
    "train_configs_file = 'train_configs.yaml'\n",
    "task_configs_file = 'task_configs.yaml'\n",
    "highres_trainer = DiffusionEdfTrainer(configs_root_dir=configs_root_dir,\n",
    "                              train_configs_file=train_configs_file,\n",
    "                              task_configs_file=task_configs_file)\n",
    "assert lowres_trainer.device == highres_trainer.device\n",
    "device = lowres_trainer.device\n",
    "\n",
    "\n",
    "lowres_trainer._init_dataloaders()\n",
    "lowres_model = lowres_trainer.get_model(checkpoint_dir='runs/2023_06_01_13-10-19_Lowres/checkpoint/200.pt',\n",
    "                                        deterministic=False, \n",
    "                                        device = device,).eval()\n",
    "highres_model = highres_trainer.get_model(checkpoint_dir='runs/2023_06_01_13-23-18_Highres/checkpoint/200.pt',\n",
    "                                         deterministic=False, \n",
    "                                         device = device,).eval()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = list(lowres_trainer.trainloader)\n",
    "dataset = list(lowres_trainer.testloader)\n",
    "demo_batch = dataset[0]\n",
    "B = len(demo_batch)\n",
    "assert B == 1, \"Batch training is not supported yet.\"\n",
    "\n",
    "\n",
    "# scene_input, grasp_input, T_target = train_utils.flatten_batch(demo_batch=demo_batch) # T_target: (Nbatch, Ngrasps, 7)\n",
    "# T_target = T_target.squeeze(0) # (B=1, N_poses=1, 7) -> (1,7) \n",
    "\n",
    "# time = torch.tensor([trainer.t_max], dtype=T_target.dtype, device=T_target.device)\n",
    "# print(f\"Diffusion time: {time.item()}\")\n",
    "# x_ref, n_neighbors = train_utils.transform_and_sample_reference_points(T_target=T_target,\n",
    "#                                                                        scene_points=scene_input,\n",
    "#                                                                        grasp_points=grasp_input,\n",
    "#                                                                        contact_radius=trainer.contact_radius,\n",
    "#                                                                        n_samples_x_ref=1)\n",
    "# T0, delta_T, time_in, gt_score, gt_score_ref = train_utils.diffuse_T_target(T_target=T_target, \n",
    "#                                                                             x_ref=x_ref, \n",
    "#                                                                             time=time, \n",
    "#                                                                             lin_mult=score_model.lin_mult)\n",
    "# (gt_ang_score, gt_lin_score), (gt_ang_score_ref, gt_lin_score_ref) = gt_score, gt_score_ref\n",
    "\n",
    "scene_input, grasp_input, _ = train_utils.flatten_batch(demo_batch=demo_batch)\n",
    "# T0 = torch.cat([\n",
    "#     transforms.random_quaternions(1, device=device),\n",
    "#     torch.distributions.Uniform(scene_input.x[:].min(dim=0).values, scene_input.x[:].max(dim=0).values).sample(sample_shape=(1,))\n",
    "# ], dim=-1)\n",
    "T0 = torch.cat([\n",
    "    #transforms.random_quaternions(1, device=device),\n",
    "    #torch.tensor([[math.sqrt(0.5), -math.sqrt(0.5), 0.0, 0.]], device=device),\n",
    "    torch.tensor([[1., 0., 0.0, 0.]], device=device),\n",
    "    torch.tensor([[-30., -30., 30.]], device=device)\n",
    "], dim=-1)\n",
    "scene_pcd = PointCloud(points=scene_input.x, colors=scene_input.f)\n",
    "grasp_pcd = PointCloud(points=grasp_input.x, colors=grasp_input.f)\n",
    "\n",
    "\n",
    "# diffused_pose_pcd = PointCloud.merge(\n",
    "#     scene_pcd,\n",
    "#     grasp_pcd.transformed(SE3(T0))[0],\n",
    "# )\n",
    "# diffused_pose_pcd.show(point_size=2., width=600, height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    scene_out_multiscale: List[FeaturedPoints] = [lowres_model.key_model(scene_input)]\n",
    "    grasp_out: FeaturedPoints = lowres_model.query_model(grasp_input)\n",
    "\n",
    "if scene_out_multiscale[0].w is not None:\n",
    "    scene_attn_pcd = PointCloud(points=scene_out_multiscale[0].x.detach().cpu(), \n",
    "                                colors=scene_out_multiscale[0].w.detach().cpu(),\n",
    "                                cmap='magma')\n",
    "    scene_attn_pcd.show(point_size=6., width=600, height=600)\n",
    "else:\n",
    "    scene_attn_pcd = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ts_lowres = lowres_model.sample(T_seed=T0.clone().detach(),\n",
    "                                scene_pcd_multiscale=scene_out_multiscale,\n",
    "                                grasp_pcd=grasp_out,\n",
    "                                diffusion_schedules=lowres_trainer.diffusion_schedules,\n",
    "                                N_steps=[500, 500],\n",
    "                                timesteps=[0.01, 0.01],\n",
    "                                add_noise=True\n",
    "                                temperature=1.)\n",
    "T_lowres = Ts_lowres[-1:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    scene_out_multiscale: List[FeaturedPoints] = highres_model.key_model(scene_input)\n",
    "    grasp_out: FeaturedPoints = highres_model.query_model(grasp_input)\n",
    "\n",
    "if scene_out_multiscale[0].w is not None:\n",
    "    scene_attn_pcd = PointCloud(points=scene_out_multiscale[0].x.detach().cpu(), \n",
    "                                colors=scene_out_multiscale[0].w.detach().cpu(),\n",
    "                                cmap='magma')\n",
    "    scene_attn_pcd.show(point_size=6., width=600, height=600)\n",
    "else:\n",
    "    scene_attn_pcd = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ts_highres = highres_model.sample(T_seed=T_lowres.clone().detach(),\n",
    "                                  scene_pcd_multiscale=scene_out_multiscale,\n",
    "                                  grasp_pcd=grasp_out,\n",
    "                                  diffusion_schedules=highres_trainer.diffusion_schedules,\n",
    "                                  N_steps=[500, 500],\n",
    "                                  timesteps=[0.01, 0.01],\n",
    "                                  add_noise=True,\n",
    "                                  temperature=1.)\n",
    "T_highres = Ts_highres[-1:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import importlib\n",
    "# import diffusion_edf.visualize\n",
    "# importlib.reload(diffusion_edf.visualize)\n",
    "# visualize_pose = diffusion_edf.visualize.visualize_pose\n",
    "\n",
    "Ts = torch.cat([Ts_lowres, Ts_highres], dim=0)\n",
    "fig_grasp, fig_sample = visualize_pose(scene_pcd, grasp_pcd, poses=SE3(Ts[::10].float()), \n",
    "                                       point_size=3.0, width=800, height=800,\n",
    "                                       ranges=torch.tensor([[-40., 40.], [-40., 40.], [0., 40.]]))\n",
    "fig_sample.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diff_edf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "79a0085b6cf04e1cff261ad12d41cff4e1530d9e68d1f8fc6bd159a2915452c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
