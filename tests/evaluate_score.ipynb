{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_JIT_USE_NNC_NOT_NVFUSER\"] = \"1\"\n",
    "from typing import List, Tuple, Optional, Union, Iterable\n",
    "\n",
    "from beartype import beartype\n",
    "import datetime\n",
    "import plotly.graph_objects as go\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Compose\n",
    "from open3d.visualization.tensorboard_plugin import summary\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from e3nn import o3\n",
    "\n",
    "from diffusion_edf.data import DemoSeqDataset, DemoSequence, TargetPoseDemo, PointCloud, SE3\n",
    "from diffusion_edf.gnn_data import FeaturedPoints, merge_featured_points, GraphEdge, flatten_featured_points, set_featured_points_attribute, _featured_points_repr\n",
    "from diffusion_edf import train_utils\n",
    "from diffusion_edf import preprocess\n",
    "from diffusion_edf import transforms\n",
    "from diffusion_edf.feature_extractor import UnetFeatureExtractor\n",
    "from diffusion_edf.tensor_field import TensorField\n",
    "from diffusion_edf.radial_func import SinusoidalPositionEmbeddings\n",
    "from diffusion_edf.equivariant_score_model import ScoreModel\n",
    "from diffusion_edf.utils import sample_reference_points\n",
    "from diffusion_edf.dist import diffuse_isotropic_se3, adjoint_inv_tr_isotropic_se3_score, diffuse_isotropic_se3_batched\n",
    "\n",
    "torch.set_printoptions(precision=4, sci_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'\n",
    "eval = True\n",
    "compile = False\n",
    "\n",
    "model_configs_dir = 'configs/test/model_configs.yaml'\n",
    "eval_configs_dir = 'configs/test/eval_configs.yaml'\n",
    "task_configs_dir = 'configs/test/task_configs.yaml'\n",
    "\n",
    "with open(model_configs_dir) as file:\n",
    "    model_configs = yaml.load(file, Loader=yaml.FullLoader)\n",
    "with open(eval_configs_dir) as file:\n",
    "    eval_configs = yaml.load(file, Loader=yaml.FullLoader)\n",
    "with open(task_configs_dir) as file:\n",
    "    task_configs = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "eval_configs['preprocess_config'].append({\n",
    "    'name': 'Rescale',\n",
    "    'kwargs': {'rescale_factor': 1/task_configs['unit_length']}\n",
    "})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_fn = []\n",
    "for proc in eval_configs['preprocess_config']:\n",
    "    proc_fn.append(\n",
    "        getattr(preprocess, proc['name'])(**proc['kwargs'])\n",
    "    )\n",
    "proc_fn = Compose(proc_fn)\n",
    "\n",
    "\n",
    "collate_fn = train_utils.get_collate_fn(task=eval_configs['task_type'], proc_fn=proc_fn)\n",
    "trainset = DemoSeqDataset(dataset_dir=eval_configs['dataset_dir'], annotation_file=eval_configs['annotation_file'], device=device)\n",
    "train_dataloader = DataLoader(trainset, shuffle=True, collate_fn=collate_fn, batch_size=eval_configs['n_batches'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_model = ScoreModel(**model_configs, deterministic=False).to(device=device)\n",
    "if compile:\n",
    "    raise NotImplementedError\n",
    "if eval:\n",
    "    score_model = score_model.eval()\n",
    "\n",
    "# optimizer = torch.optim.Adam(list(score_model.parameters()), **train_configs['optimizer_kwargs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_file: Optional[str] = eval_configs['checkpoint_file']\n",
    "task_type = eval_configs['task_type']\n",
    "\n",
    "if checkpoint_file is not None:\n",
    "    checkpoint = torch.load(checkpoint_file)\n",
    "    score_model.load_state_dict(checkpoint['score_model_state_dict'])\n",
    "    # optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    steps = checkpoint['steps']\n",
    "    print(f\"Successfully Loaded checkpoint @ epoch: {epoch} (steps: {steps})\")\n",
    "else:\n",
    "    print(f\"Initialize without loading from checkpoint.\")\n",
    "    epoch = 0\n",
    "    steps = 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contact_radius = eval_configs['contact_radius']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for demo_batch in train_dataloader:\n",
    "    B = len(demo_batch)\n",
    "    assert B == 1, \"Batch training is not supported yet.\"\n",
    "\n",
    "    scene_input, grasp_input, T_target = train_utils.flatten_batch(demo_batch=demo_batch) # T_target: (Nbatch, Ngrasps, 7)\n",
    "    T_target = T_target.squeeze(0) # (B=1, N_poses=1, 7) -> (1,7) \n",
    "\n",
    "    # min_time = torch.tensor([1e-3], dtype=T_target.dtype, device=T_target.device)\n",
    "    # max_time = torch.tensor([1.0], dtype=T_target.dtype, device=T_target.device)\n",
    "    # time_in = (min_time/max_time + torch.rand(1, dtype=T_target.dtype, device=T_target.device) * (1-min_time/max_time))*max_time   # Shape: (1,)\n",
    "    # #time_in = torch.exp(torch.rand_like(max_time) * (torch.log(max_time)-torch.log(min_time)) + torch.log(min_time))              # Shape: (1,)\n",
    "    time_in = torch.tensor([1.0], dtype=T_target.dtype, device=T_target.device)\n",
    "\n",
    "    eps = time_in / 2   # Shape: (1,)\n",
    "    std = torch.sqrt(time_in) * score_model.lin_mult   # Shape: (1,)\n",
    "    x_ref, n_neighbors = sample_reference_points(\n",
    "        src_points = PointCloud(points=scene_input.x, colors=scene_input.f).transformed(\n",
    "                                SE3(T_target).inv(), squeeze=True\n",
    "                                ).points, \n",
    "        dst_points = grasp_input.x, \n",
    "        r=contact_radius, \n",
    "        n_samples=1\n",
    "    )\n",
    "    # T, delta_T, (gt_ang_score, gt_lin_score), (gt_ang_score_ref, gt_lin_score_ref) = diffuse_isotropic_se3(T0 = T_target, eps=eps, std=std, x_ref=x_ref, double_precision=True)\n",
    "    T, delta_T, (gt_ang_score, gt_lin_score), (gt_ang_score_ref, gt_lin_score_ref) = diffuse_isotropic_se3_batched(T0 = T_target, eps=eps, std=std, x_ref=x_ref, double_precision=True)\n",
    "    T, delta_T, (gt_ang_score, gt_lin_score), (gt_ang_score_ref, gt_lin_score_ref) = T.squeeze(-2), delta_T.squeeze(-2), (gt_ang_score.squeeze(-2), gt_lin_score.squeeze(-2)), (gt_ang_score_ref.squeeze(-2), gt_lin_score_ref.squeeze(-2))\n",
    "    # T: (nT, 7) || delta_T: (nT, 7) || gt_*_score_*: (nT, 3) ||\n",
    "    # Note that nT = n_samples_x_ref * nT_target  ||   nT_target = 1\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    scene_out: FeaturedPoints = score_model.key_feature_extractor(scene_input)\n",
    "    grasp_out: FeaturedPoints = score_model.query_feature_extractor(grasp_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_steps = 500\n",
    "temp = 1.\n",
    "\n",
    "# t_schedule = torch.exp(torch.linspace(math.log(1.), math.log(1e-2), N_steps+1, device=device)).unsqueeze(-1)\n",
    "# t_schedule = torch.cat([t_schedule, t_schedule[-1].expand(100,1)], dim=-2)\n",
    "\n",
    "t_schedule = torch.linspace(1., 1e-3, N_steps+1, device=device).unsqueeze(-1)\n",
    "noise_schedule_ang = torch.ones_like(t_schedule)\n",
    "noise_schedule_lin = torch.ones_like(t_schedule) * score_model.lin_mult\n",
    "dt = torch.tensor([0.001], dtype=T.dtype, device=T.device)\n",
    "\n",
    "\n",
    "T_shape = T.shape\n",
    "T_next = T \n",
    "for i in tqdm(range(len(t_schedule)-1)):\n",
    "    t = t_schedule[i]\n",
    "    noise_level_ang = noise_schedule_ang[i]\n",
    "    noise_level_lin = noise_schedule_lin[i]\n",
    "    with torch.no_grad():\n",
    "        (ang_score, lin_score) = score_model.score_head(Ts=T_next.view(-1,7), \n",
    "                                                        key_pcd=scene_out,\n",
    "                                                        query_pcd=grasp_out,\n",
    "                                                        time = t.repeat(len(T_next)))\n",
    "    ang_score = ang_score / torch.sqrt(dt)\n",
    "    lin_score = lin_score / torch.sqrt(dt)\n",
    "\n",
    "    ang_disp = ang_score * (torch.square(noise_level_ang) / temp * dt) + (torch.randn_like(ang_score) * (noise_level_ang * torch.sqrt(dt)))\n",
    "    lin_disp = lin_score * (torch.square(noise_level_lin) / temp * dt) + (torch.randn_like(lin_score) * (noise_level_lin * torch.sqrt(dt)))\n",
    "\n",
    "    L = T_next.detach()[...,score_model.q_indices] * score_model.q_factor\n",
    "    q, x = T_next[...,:4], T_next[...,4:]\n",
    "    dq = torch.einsum('...ij,...j->...i', L, ang_disp)\n",
    "    dx = transforms.quaternion_apply(q, lin_disp)\n",
    "    q_next = transforms.normalize_quaternion(q + dq)\n",
    "    T_next = torch.cat([q_next, x+dx], dim=-1)\n",
    "\n",
    "    # dT = transforms.se3_exp_map(torch.cat([lin_disp, ang_disp], dim=-1))\n",
    "    # dT = torch.cat([transforms.matrix_to_quaternion(dT[..., :3, :3]), dT[..., :3, 3]], dim=-1)\n",
    "    # T_next = transforms.multiply_se3(T_next, dT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    scene_pcd = PointCloud(points=scene_input.x, colors=scene_input.f)\n",
    "    grasp_pcd = PointCloud(points=grasp_input.x, colors=grasp_input.f)\n",
    "    target_pose_pcd = PointCloud.merge(\n",
    "        scene_pcd,\n",
    "        grasp_pcd.transformed(SE3(T_target), squeeze=True),\n",
    "    )\n",
    "    diffused_pose_pcd = PointCloud.merge(\n",
    "        scene_pcd,\n",
    "        grasp_pcd.transformed(SE3(T))[0],\n",
    "    )\n",
    "    denoised_pose_pcd = PointCloud.merge(\n",
    "        scene_pcd,\n",
    "        grasp_pcd.transformed(SE3(T_next.detach()))[0],\n",
    "    )\n",
    "    \n",
    "    ##### Query Summary #####\n",
    "    query_weight, query_points, query_point_batch = grasp_out.w.detach(), grasp_out.x.detach(), grasp_out.b.detach(), \n",
    "    batch_vis_idx = (query_point_batch == 0).nonzero().squeeze(-1)\n",
    "    query_weight, query_points = query_weight[batch_vis_idx], query_points[batch_vis_idx]\n",
    "\n",
    "    N_repeat = 500\n",
    "    query_points_colors = torch.tensor([0.01, 1., 1.], device=query_weight.device, dtype=query_weight.dtype).expand(N_repeat, 1, 3) * query_weight[None, :, None]\n",
    "    r_query_ball = 0.5\n",
    "\n",
    "    ball = torch.randn(N_repeat,1,3, device=query_points.device, dtype=query_points.dtype)\n",
    "    ball = ball/ball.norm(dim=-1, keepdim=True) * r_query_ball\n",
    "    query_points = (query_points + ball).reshape(-1,3)\n",
    "    query_points_colors = query_points_colors.reshape(-1,3)\n",
    "\n",
    "    #########################\n",
    "    scene_attn_pcd = PointCloud(points=scene_out.x.detach().cpu(), \n",
    "                                colors=scene_out.w.detach().cpu(),\n",
    "                                cmap='magma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_pose_pcd.show(point_size=2., width=800, height=800)\n",
    "# diffused_pose_pcd.show(point_size=2., width=800, height=800)\n",
    "# denoised_pose_pcd.show(point_size=2., width=800, height=800)\n",
    "scene_attn_pcd.show(point_size=3., width=800, height=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diff_edf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "79a0085b6cf04e1cff261ad12d41cff4e1530d9e68d1f8fc6bd159a2915452c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
