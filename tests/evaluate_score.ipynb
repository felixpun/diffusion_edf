{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_JIT_USE_NNC_NOT_NVFUSER\"] = \"1\"\n",
    "from typing import List, Tuple, Optional, Union, Iterable\n",
    "import math\n",
    "\n",
    "from beartype import beartype\n",
    "import datetime\n",
    "import plotly.graph_objects as go\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Compose\n",
    "from e3nn import o3\n",
    "\n",
    "from diffusion_edf.data import DemoSeqDataset, DemoSequence, TargetPoseDemo, PointCloud, SE3\n",
    "from diffusion_edf.gnn_data import FeaturedPoints, merge_featured_points, GraphEdge, flatten_featured_points, set_featured_points_attribute, _featured_points_repr, detach_featured_points\n",
    "from diffusion_edf import train_utils\n",
    "from diffusion_edf import transforms\n",
    "from diffusion_edf.utils import sample_reference_points\n",
    "from diffusion_edf.dist import diffuse_isotropic_se3, adjoint_inv_tr_isotropic_se3_score, diffuse_isotropic_se3_batched\n",
    "from diffusion_edf.point_attentive_score_model import PointAttentiveScoreModel\n",
    "from diffusion_edf.trainer import DiffusionEdfTrainer\n",
    "from diffusion_edf.visualize import visualize_pose\n",
    "\n",
    "torch.set_printoptions(precision=4, sci_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs_root_dir = 'configs/pick_point_attn'\n",
    "train_configs_file = 'train_configs.yaml'\n",
    "task_configs_file = 'task_configs.yaml'\n",
    "trainer = DiffusionEdfTrainer(configs_root_dir=configs_root_dir,\n",
    "                              train_configs_file=train_configs_file,\n",
    "                              task_configs_file=task_configs_file)\n",
    "device = trainer.device\n",
    "\n",
    "trainer._init_dataloaders()\n",
    "score_model = trainer.get_model(checkpoint_dir='runs/2023_05_30_17-05-29_MultiphaseTraining/checkpoint/200.pt',\n",
    "                                deterministic=False, \n",
    "                                device = trainer.device,).eval()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = list(trainer.trainloader)\n",
    "# dataset = list(trainer.testloader)\n",
    "demo_batch = dataset[1]\n",
    "B = len(demo_batch)\n",
    "assert B == 1, \"Batch training is not supported yet.\"\n",
    "\n",
    "\n",
    "# scene_input, grasp_input, T_target = train_utils.flatten_batch(demo_batch=demo_batch) # T_target: (Nbatch, Ngrasps, 7)\n",
    "# T_target = T_target.squeeze(0) # (B=1, N_poses=1, 7) -> (1,7) \n",
    "\n",
    "# time = torch.tensor([trainer.t_max], dtype=T_target.dtype, device=T_target.device)\n",
    "# print(f\"Diffusion time: {time.item()}\")\n",
    "# x_ref, n_neighbors = train_utils.transform_and_sample_reference_points(T_target=T_target,\n",
    "#                                                                        scene_points=scene_input,\n",
    "#                                                                        grasp_points=grasp_input,\n",
    "#                                                                        contact_radius=trainer.contact_radius,\n",
    "#                                                                        n_samples_x_ref=1)\n",
    "# T0, delta_T, time_in, gt_score, gt_score_ref = train_utils.diffuse_T_target(T_target=T_target, \n",
    "#                                                                             x_ref=x_ref, \n",
    "#                                                                             time=time, \n",
    "#                                                                             lin_mult=score_model.lin_mult)\n",
    "# (gt_ang_score, gt_lin_score), (gt_ang_score_ref, gt_lin_score_ref) = gt_score, gt_score_ref\n",
    "\n",
    "scene_input, grasp_input, _ = train_utils.flatten_batch(demo_batch=demo_batch)\n",
    "T0 = torch.cat([\n",
    "    transforms.random_quaternions(1, device=device),\n",
    "    torch.distributions.Uniform(scene_input.x[:].min(dim=0).values, scene_input.x[:].max(dim=0).values).sample(sample_shape=(1,))\n",
    "], dim=-1)\n",
    "\n",
    "\n",
    "scene_pcd = PointCloud(points=scene_input.x, colors=scene_input.f)\n",
    "grasp_pcd = PointCloud(points=grasp_input.x, colors=grasp_input.f)\n",
    "# diffused_pose_pcd = PointCloud.merge(\n",
    "#     scene_pcd,\n",
    "#     grasp_pcd.transformed(SE3(T0))[0],\n",
    "# )\n",
    "# diffused_pose_pcd.show(point_size=2., width=600, height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    scene_out_multiscale: List[FeaturedPoints] = [score_model.key_model(scene_input)]\n",
    "    grasp_out: FeaturedPoints = score_model.query_model(grasp_input)\n",
    "    \n",
    "scene_attn_pcd = PointCloud(points=scene_out_multiscale[0].x.detach().cpu(), \n",
    "                            colors=scene_out_multiscale[0].w.detach().cpu(),\n",
    "                            cmap='magma')\n",
    "scene_attn_pcd.show(point_size=6., width=600, height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = T0.clone().detach()\n",
    "T_shape = T.shape\n",
    "\n",
    "N_steps_per_phase = [500, 500, 500, 500]\n",
    "steps_per_record = 10\n",
    "temp = 0.5\n",
    "\n",
    "Ts = [T.clone().detach()]\n",
    "steps = 0\n",
    "for n_steps, schedule in zip(N_steps_per_phase, trainer.diffusion_schedules):\n",
    "    t_schedule = torch.linspace(schedule[0], schedule[1], n_steps, device=device)\n",
    "    # step_schedule = t_schedule/t_schedule.max() * 0.1\n",
    "    step_schedule = torch.ones_like(t_schedule) * 0.001 * t_schedule.max()\n",
    "    t_schedule = t_schedule.unsqueeze(-1)\n",
    "\n",
    "    for i in tqdm(range(len(t_schedule))):\n",
    "        t = t_schedule[i]\n",
    "        dt = t_schedule[i]\n",
    "        with torch.no_grad():\n",
    "            (ang_score, lin_score) = score_model.score_head(Ts=T.view(-1,7), \n",
    "                                                            key_pcd_multiscale=scene_out_multiscale,\n",
    "                                                            query_pcd=grasp_out,\n",
    "                                                            time = t.repeat(len(T)))\n",
    "        ang_score = ang_score / torch.sqrt(t)\n",
    "        lin_score = lin_score / torch.sqrt(t) / score_model.lin_mult\n",
    "\n",
    "        ang_disp = ang_score * dt / (2*temp) + (torch.randn_like(ang_score) * torch.sqrt(dt))\n",
    "        lin_disp = lin_score * dt / (2*temp) + (torch.randn_like(lin_score) * torch.sqrt(dt))\n",
    "\n",
    "        L = T.detach()[...,score_model.q_indices] * score_model.q_factor\n",
    "        q, x = T[...,:4], T[...,4:]\n",
    "        dq = torch.einsum('...ij,...j->...i', L, ang_disp)\n",
    "        dx = transforms.quaternion_apply(q, lin_disp)\n",
    "        q = transforms.normalize_quaternion(q + dq)\n",
    "        T = torch.cat([q, x+dx], dim=-1)\n",
    "\n",
    "        # dT = transforms.se3_exp_map(torch.cat([lin_disp, ang_disp], dim=-1))\n",
    "        # dT = torch.cat([transforms.matrix_to_quaternion(dT[..., :3, :3]), dT[..., :3, 3]], dim=-1)\n",
    "        # T = transforms.multiply_se3(T, dT)\n",
    "        steps += 1\n",
    "        if steps % steps_per_record == 0:\n",
    "            Ts.append(T.clone().detach())\n",
    "\n",
    "Ts.append(T.clone().detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import importlib\n",
    "# import diffusion_edf.visualize\n",
    "# importlib.reload(diffusion_edf.visualize)\n",
    "# visualize_pose = diffusion_edf.visualize.visualize_pose\n",
    "fig_grasp, fig_sample = visualize_pose(scene_pcd, grasp_pcd, poses=SE3(torch.cat(Ts, dim=0).detach()), point_size=3.0, width=1000, height=1000)\n",
    "fig_sample.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diff_edf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "79a0085b6cf04e1cff261ad12d41cff4e1530d9e68d1f8fc6bd159a2915452c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
