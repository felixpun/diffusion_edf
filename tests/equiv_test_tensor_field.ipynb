{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_JIT_USE_NNC_NOT_NVFUSER\"] = \"1\"\n",
    "from typing import List, Tuple, Optional, Union, Iterable, NamedTuple, Any, Sequence\n",
    "\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "\n",
    "import torch\n",
    "from torchvision.transforms import Compose\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from e3nn import o3\n",
    "\n",
    "from diffusion_edf.data import DemoSeqDataset, DemoSequence, TargetPoseDemo, PointCloud\n",
    "from diffusion_edf.gnn_data import FeaturedPoints, merge_featured_points, GraphEdge\n",
    "from diffusion_edf import train_utils\n",
    "from diffusion_edf import preprocess\n",
    "from diffusion_edf.feature_extractor import UnetFeatureExtractor\n",
    "from diffusion_edf.tensor_field import TensorField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'\n",
    "task = 'pick'\n",
    "eval = True\n",
    "compile = False\n",
    "\n",
    "model_configs_dir = 'configs/test/model_configs.yaml'\n",
    "train_configs_dir = 'configs/test/train_configs.yaml'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load configs, preprocessor, and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(model_configs_dir) as file:\n",
    "    model_configs = yaml.load(file, Loader=yaml.FullLoader)\n",
    "with open(train_configs_dir) as file:\n",
    "    train_configs = yaml.load(file, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_fn = []\n",
    "for proc in train_configs['preprocess_config']:\n",
    "    proc_fn.append(\n",
    "        getattr(preprocess, proc['name'])(**proc['configs'])\n",
    "    )\n",
    "proc_fn = Compose(proc_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_fn = train_utils.get_collate_fn(task=task, proc_fn=proc_fn)\n",
    "trainset = DemoSeqDataset(dataset_dir=train_configs['dataset_dir'], annotation_file=train_configs['annotation_file'], device=device)\n",
    "train_dataloader = DataLoader(trainset, shuffle=True, collate_fn=collate_fn, batch_size=train_configs['n_batches'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fe_kwargs = model_configs['scene']['feature_extractor_configs']\n",
    "tf_kwargs = model_configs['scene']['tensor_field_configs']\n",
    "\n",
    "feature_extractor = UnetFeatureExtractor(**fe_kwargs).to(device)\n",
    "tf_kwargs['irreps_input'] = str(feature_extractor.irreps_output)\n",
    "tf = tensor_field = TensorField(**tf_kwargs).to(device)\n",
    "if compile:\n",
    "    feature_extractor = torch.jit.script(feature_extractor)\n",
    "    tf = torch.jit.script(tf)\n",
    "if eval:\n",
    "    feature_extractor = feature_extractor.eval()\n",
    "    tf = tf.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loop Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for demo_batch in train_dataloader:\n",
    "    B = len(demo_batch)\n",
    "    scene_pcd, grasp_pcd, target_poses = train_utils.flatten_batch(demo_batch=demo_batch) # target_poses: (Nbatch, Ngrasps, 7)\n",
    "    input_pcd = FeaturedPoints(\n",
    "        x=scene_pcd.x/model_configs['unit_len'],\n",
    "        f=scene_pcd.f,\n",
    "        b=scene_pcd.b\n",
    "    )\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = feature_extractor(input_pcd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nq = 20\n",
    "query_x = output[0].x.detach().mean(dim=0).expand(Nq,-1)\n",
    "query_x = query_x + (0.1 * torch.randn_like(query_x) * output[0].x.detach().std(dim=0))\n",
    "query_points = FeaturedPoints(x=query_x, f=torch.empty_like(query_x), b=torch.zeros(len(query_x), dtype=torch.long, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_output = tf(query_points,\n",
    "                  input_points = output[-1],\n",
    "                  time_emb = torch.randn(B,tf.time_emb_dim, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdfa"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rotate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale = 3\n",
    "# node_coord = output[scale].x\n",
    "# node_feature = output[scale].f\n",
    "# node_batch = output[scale].b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusion_edf.transforms import quaternion_apply, random_quaternions\n",
    "from diffusion_edf.gnn_data import TransformPcd\n",
    "\n",
    "transform_input = torch.jit.script(TransformPcd(irreps=model_configs['scene']['feature_extractor_configs']['irreps_input'], device=device))\n",
    "transform_output = torch.jit.script(TransformPcd(irreps=model_configs['scene']['feature_extractor_configs']['irreps_output'], device=device))\n",
    "\n",
    "rot = random_quaternions(1, device=device)\n",
    "trans = torch.randn(1,3,device=device)\n",
    "Ts = torch.cat([rot,trans], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_idx = 0\n",
    "scale_idx = 3\n",
    "\n",
    "input_pcd_rot: FeaturedPoints = transform_input(input_pcd, Ts)\n",
    "input_pcd_rot = FeaturedPoints(x=input_pcd_rot.x[T_idx], f=input_pcd_rot.f[T_idx], b=input_pcd_rot.b[T_idx])\n",
    "\n",
    "pre_rot_out = feature_extractor(input_pcd_rot)[scale_idx]\n",
    "post_rot_out: FeaturedPoints = transform_output(output[scale_idx], Ts)\n",
    "post_rot_out = FeaturedPoints(x=post_rot_out.x[T_idx], f=post_rot_out.f[T_idx], b=post_rot_out.b[T_idx], w=post_rot_out.w[T_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isclose = torch.isclose(pre_rot_out.x, post_rot_out.x, atol=0.01, rtol=0.01)\n",
    "print(f\"Equivariance ratio: {(isclose.sum() / len(pre_rot_out.x.view(-1))).item()}\") # Slight non-equivariance comes from FPS downsampling algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isclose = torch.isclose(pre_rot_out.f, post_rot_out.f, atol=0.001, rtol=0.001)\n",
    "print(f\"Equivariance ratio: {(isclose.sum() / len(pre_rot_out.f.view(-1))).item()}\") # Slight non-equivariance comes from FPS downsampling algorithm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diff_edf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "79a0085b6cf04e1cff261ad12d41cff4e1530d9e68d1f8fc6bd159a2915452c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
