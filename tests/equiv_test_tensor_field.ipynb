{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_JIT_USE_NNC_NOT_NVFUSER\"] = \"1\"\n",
    "from typing import List, Tuple, Optional, Union, Iterable, NamedTuple, Any, Sequence\n",
    "\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "\n",
    "import torch\n",
    "from torchvision.transforms import Compose\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from e3nn import o3\n",
    "\n",
    "from diffusion_edf.data import DemoSeqDataset, DemoSequence, TargetPoseDemo, PointCloud\n",
    "from diffusion_edf.gnn_data import FeaturedPoints, merge_featured_points, GraphEdge, flatten_featured_points\n",
    "from diffusion_edf import train_utils\n",
    "from diffusion_edf import preprocess\n",
    "from diffusion_edf.feature_extractor import UnetFeatureExtractor\n",
    "from diffusion_edf.tensor_field import TensorField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'\n",
    "task = 'pick'\n",
    "eval = True\n",
    "compile = False\n",
    "\n",
    "model_configs_dir = 'configs/test/model_configs.yaml'\n",
    "train_configs_dir = 'configs/test/train_configs.yaml'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load configs, preprocessor, and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(model_configs_dir) as file:\n",
    "    model_configs = yaml.load(file, Loader=yaml.FullLoader)\n",
    "with open(train_configs_dir) as file:\n",
    "    train_configs = yaml.load(file, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_fn = []\n",
    "for proc in train_configs['preprocess_config']:\n",
    "    proc_fn.append(\n",
    "        getattr(preprocess, proc['name'])(**proc['configs'])\n",
    "    )\n",
    "proc_fn = Compose(proc_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_fn = train_utils.get_collate_fn(task=task, proc_fn=proc_fn)\n",
    "trainset = DemoSeqDataset(dataset_dir=train_configs['dataset_dir'], annotation_file=train_configs['annotation_file'], device=device)\n",
    "train_dataloader = DataLoader(trainset, shuffle=True, collate_fn=collate_fn, batch_size=train_configs['n_batches'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hw/anaconda3/envs/diff_edf/lib/python3.8/site-packages/torch/jit/_check.py:181: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\"The TorchScript type system doesn't support \"\n"
     ]
    }
   ],
   "source": [
    "fe_kwargs = model_configs['scene']['feature_extractor_configs']\n",
    "tf_kwargs = model_configs['scene']['tensor_field_configs']\n",
    "\n",
    "feature_extractor = UnetFeatureExtractor(**fe_kwargs, deterministic=True).to(device)\n",
    "tf_kwargs['irreps_input'] = str(feature_extractor.irreps_output)\n",
    "tf = tensor_field = TensorField(**tf_kwargs).to(device)\n",
    "if compile:\n",
    "    feature_extractor = torch.jit.script(feature_extractor)\n",
    "    tf = torch.jit.script(tf)\n",
    "if eval:\n",
    "    feature_extractor = feature_extractor.eval()\n",
    "    tf = tf.eval()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loop Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for demo_batch in train_dataloader:\n",
    "    B = len(demo_batch)\n",
    "    scene_pcd, grasp_pcd, target_poses = train_utils.flatten_batch(demo_batch=demo_batch) # target_poses: (Nbatch, Ngrasps, 7)\n",
    "    input_pcd = FeaturedPoints(\n",
    "        x=scene_pcd.x/model_configs['unit_len'],\n",
    "        f=scene_pcd.f,\n",
    "        b=scene_pcd.b\n",
    "    )\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nq = 20\n",
    "query_x = input_pcd.x.detach().mean(dim=0).expand(Nq,-1)\n",
    "query_x = query_x + (0.1 * torch.randn_like(query_x) * input_pcd.x.detach().std(dim=0))\n",
    "query_points = FeaturedPoints(x=query_x, f=torch.empty_like(query_x), b=torch.zeros(len(query_x), dtype=torch.long, device=device))\n",
    "time_emb = torch.randn(B,tf.time_emb_dim, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_points_multiscale = feature_extractor(input_pcd)\n",
    "# output_points = output_points_multiscale[scale_idx]\n",
    "output_points = feature_extractor(input_pcd)\n",
    "field_output = tf(query_points,\n",
    "                  input_points = output_points,\n",
    "                  time_emb = time_emb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rotate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusion_edf.transforms import quaternion_apply, random_quaternions\n",
    "from diffusion_edf.gnn_data import TransformPcd\n",
    "\n",
    "transform_input = torch.jit.script(TransformPcd(irreps=model_configs['scene']['feature_extractor_configs']['irreps_input']).to(device=device))\n",
    "transform_output = torch.jit.script(TransformPcd(irreps=model_configs['scene']['feature_extractor_configs']['irreps_output']).to(device=device))\n",
    "transform_field = torch.jit.script(TransformPcd(irreps=model_configs['scene']['tensor_field_configs']['irreps_output']).to(device=device))\n",
    "\n",
    "rot = random_quaternions(1, device=device)\n",
    "trans = torch.randn(1,3,device=device)\n",
    "Ts = torch.cat([rot,trans], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_pcd_rot: FeaturedPoints = flatten_featured_points(transform_input(input_pcd, Ts))\n",
    "query_pcd_rot: FeaturedPoints = flatten_featured_points(transform_input(query_points, Ts))\n",
    "post_rot_output_points: FeaturedPoints = flatten_featured_points(transform_output(output_points, Ts))\n",
    "post_rot_field_output: FeaturedPoints = flatten_featured_points(transform_field(field_output, Ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position Equivariance ratio: 0.9966805577278137\n",
      "Feature Equivariance ratio: 0.9966943264007568\n"
     ]
    }
   ],
   "source": [
    "# pre_rot_output_points = feature_extractor(input_pcd_rot)[scale_idx]\n",
    "pre_rot_output_points = feature_extractor(input_pcd_rot)\n",
    "\n",
    "isclose = torch.isclose(pre_rot_output_points.x, post_rot_output_points.x, atol=0.01, rtol=0.01)\n",
    "print(f\"Position Equivariance ratio: {(isclose.sum() / len(pre_rot_output_points.x.view(-1))).item()}\") # Slight non-equivariance comes from FPS downsampling algorithm.\n",
    "isclose = torch.isclose(pre_rot_output_points.f, post_rot_output_points.f, atol=0.001, rtol=0.001)\n",
    "print(f\"Feature Equivariance ratio: {(isclose.sum() / len(pre_rot_output_points.f.view(-1))).item()}\") # Slight non-equivariance comes from FPS downsampling algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_rot_field_output = tf(query_pcd_rot,\n",
    "                          input_points = post_rot_output_points,\n",
    "                          time_emb = time_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position Equivariance ratio: 1.0\n",
      "Feature Equivariance ratio: 1.0\n"
     ]
    }
   ],
   "source": [
    "isclose = torch.isclose(pre_rot_field_output.x, post_rot_field_output.x, atol=0.01, rtol=0.01)\n",
    "print(f\"Position Equivariance ratio: {(isclose.sum() / len(pre_rot_field_output.x.view(-1))).item()}\") # Slight non-equivariance comes from FPS downsampling algorithm.\n",
    "isclose = torch.isclose(pre_rot_field_output.f, post_rot_field_output.f, atol=0.001, rtol=0.001)\n",
    "print(f\"Feature Equivariance ratio: {(isclose.sum() / len(pre_rot_field_output.f.view(-1))).item()}\") # Slight non-equivariance comes from FPS downsampling algorithm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diff_edf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "79a0085b6cf04e1cff261ad12d41cff4e1530d9e68d1f8fc6bd159a2915452c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
