{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_JIT_USE_NNC_NOT_NVFUSER\"] = \"1\"\n",
    "from typing import List, Tuple, Optional, Union, Iterable, NamedTuple, Any, Sequence\n",
    "\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "\n",
    "import torch\n",
    "from torchvision.transforms import Compose\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from e3nn import o3\n",
    "\n",
    "from diffusion_edf.data import DemoSeqDataset, DemoSequence, TargetPoseDemo, PointCloud\n",
    "from diffusion_edf.gnn_data import FeaturedPoints, merge_featured_points, GraphEdge, flatten_featured_points, set_featured_points_attribute, _featured_points_repr\n",
    "from diffusion_edf import train_utils\n",
    "from diffusion_edf import preprocess\n",
    "from diffusion_edf import transforms\n",
    "from diffusion_edf.feature_extractor import UnetFeatureExtractor\n",
    "from diffusion_edf.tensor_field import TensorField\n",
    "from diffusion_edf.radial_func import SinusoidalPositionEmbeddings\n",
    "from diffusion_edf.equivariant_score_model import ScoreModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'\n",
    "task = 'pick'\n",
    "eval = True\n",
    "compile = False\n",
    "\n",
    "model_configs_dir = 'configs/test/model_configs.yaml'\n",
    "train_configs_dir = 'configs/test/train_configs.yaml'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load configs, preprocessor, and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(model_configs_dir) as file:\n",
    "    model_configs = yaml.load(file, Loader=yaml.FullLoader)\n",
    "with open(train_configs_dir) as file:\n",
    "    train_configs = yaml.load(file, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_fn = []\n",
    "for proc in train_configs['preprocess_config']:\n",
    "    proc_fn.append(\n",
    "        getattr(preprocess, proc['name'])(**proc['configs'])\n",
    "    )\n",
    "proc_fn = Compose(proc_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_fn = train_utils.get_collate_fn(task=task, proc_fn=proc_fn)\n",
    "trainset = DemoSeqDataset(dataset_dir=train_configs['dataset_dir'], annotation_file=train_configs['annotation_file'], device=device)\n",
    "train_dataloader = DataLoader(trainset, shuffle=True, collate_fn=collate_fn, batch_size=train_configs['n_batches'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ScoreModel: Initializing Score Head\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hw/anaconda3/envs/diff_edf/lib/python3.8/site-packages/torch/jit/_check.py:181: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\"The TorchScript type system doesn't support \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ScoreModel: Initializing Key Feature Extractor\n",
      "ScoreModel: Initializing Query Feature Extractor\n"
     ]
    }
   ],
   "source": [
    "score_model = ScoreModel(**model_configs['score_model_configs']).to(device=device)\n",
    "if compile:\n",
    "    raise NotImplementedError\n",
    "if eval:\n",
    "    score_model = score_model.eval()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loop Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for demo_batch in train_dataloader:\n",
    "    B = len(demo_batch)\n",
    "    scene_pcd, grasp_pcd, target_poses = train_utils.flatten_batch(demo_batch=demo_batch) # target_poses: (Nbatch, Ngrasps, 7)\n",
    "    scene_pcd_rescaled = set_featured_points_attribute(scene_pcd, x=scene_pcd.x/model_configs['unit_len'])\n",
    "    grasp_pcd_rescaled = set_featured_points_attribute(grasp_pcd, x=grasp_pcd.x/model_configs['unit_len'])\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_T = 5\n",
    "q = transforms.random_quaternions(N_T, device=device)\n",
    "x = torch.randn(N_T,3,device=device)\n",
    "Ts = torch.cat([q, x], dim=-1)\n",
    "\n",
    "time = torch.rand(N_T,device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    score, debug_output = score_model(Ts=Ts, time=time,\n",
    "                                    key_pcd=scene_pcd_rescaled, \n",
    "                                    query_pcd=grasp_pcd_rescaled, \n",
    "                                    extract_features = True,\n",
    "                                    debug = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_fe_out, query_fe_out = debug_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_fe_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sfda' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sfda\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sfda' is not defined"
     ]
    }
   ],
   "source": [
    "sfda"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rotate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusion_edf.transforms import quaternion_apply, random_quaternions\n",
    "from diffusion_edf.gnn_data import TransformPcd\n",
    "\n",
    "transform_input = torch.jit.script(TransformPcd(irreps=model_configs['scene']['feature_extractor_configs']['irreps_input']).to(device=device))\n",
    "transform_query = torch.jit.script(TransformPcd(irreps=o3.Irreps(f\"{model_configs['time_emb_dim']}x0e\")).to(device=device))\n",
    "transform_output = torch.jit.script(TransformPcd(irreps=model_configs['scene']['feature_extractor_configs']['irreps_output']).to(device=device))\n",
    "transform_field = torch.jit.script(TransformPcd(irreps=model_configs['scene']['tensor_field_configs']['irreps_output']).to(device=device))\n",
    "\n",
    "rot = random_quaternions(1, device=device)\n",
    "trans = torch.randn(1,3,device=device)\n",
    "Ts = torch.cat([rot,trans], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_pcd_rot: FeaturedPoints = flatten_featured_points(transform_input(input_pcd, Ts))\n",
    "query_pcd_rot: FeaturedPoints = flatten_featured_points(transform_query(query_points, Ts))\n",
    "post_rot_output_points: FeaturedPoints = flatten_featured_points(transform_output(output_points, Ts))\n",
    "post_rot_field_output: FeaturedPoints = flatten_featured_points(transform_field(field_output, Ts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre_rot_output_points = feature_extractor(input_pcd_rot)[scale_idx]\n",
    "pre_rot_output_points = feature_extractor(input_pcd_rot)\n",
    "\n",
    "isclose = torch.isclose(pre_rot_output_points.x, post_rot_output_points.x, atol=0.01, rtol=0.01)\n",
    "print(f\"Position Equivariance ratio: {(isclose.sum() / len(pre_rot_output_points.x.view(-1))).item()}\") # Slight non-equivariance comes from FPS downsampling algorithm.\n",
    "isclose = torch.isclose(pre_rot_output_points.f, post_rot_output_points.f, atol=0.001, rtol=0.001)\n",
    "print(f\"Feature Equivariance ratio: {(isclose.sum() / len(pre_rot_output_points.f.view(-1))).item()}\") # Slight non-equivariance comes from FPS downsampling algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_rot_field_output = tensor_field(query_pcd_rot,\n",
    "                                    input_points = post_rot_output_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isclose = torch.isclose(pre_rot_field_output.x, post_rot_field_output.x, atol=0.01, rtol=0.01)\n",
    "print(f\"Position Equivariance ratio: {(isclose.sum() / len(pre_rot_field_output.x.view(-1))).item()}\") # Slight non-equivariance comes from FPS downsampling algorithm.\n",
    "isclose = torch.isclose(pre_rot_field_output.f, post_rot_field_output.f, atol=0.001, rtol=0.001)\n",
    "print(f\"Feature Equivariance ratio: {(isclose.sum() / len(pre_rot_field_output.f.view(-1))).item()}\") # Slight non-equivariance comes from FPS downsampling algorithm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diff_edf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "79a0085b6cf04e1cff261ad12d41cff4e1530d9e68d1f8fc6bd159a2915452c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
