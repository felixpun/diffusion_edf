{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_JIT_USE_NNC_NOT_NVFUSER\"] = \"1\"\n",
    "from typing import List, Tuple, Optional, Union, Iterable, NamedTuple, Any, Sequence, Dict\n",
    "\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "\n",
    "import torch\n",
    "from torchvision.transforms import Compose\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from e3nn import o3\n",
    "\n",
    "from diffusion_edf.data import DemoSeqDataset, DemoSequence, TargetPoseDemo, PointCloud\n",
    "from diffusion_edf.gnn_data import FeaturedPoints, merge_featured_points, GraphEdge, flatten_featured_points, set_featured_points_attribute, _featured_points_repr, detach_featured_points\n",
    "from diffusion_edf import train_utils\n",
    "from diffusion_edf import preprocess\n",
    "from diffusion_edf import transforms\n",
    "from diffusion_edf.feature_extractor import UnetFeatureExtractor\n",
    "from diffusion_edf.radial_func import SinusoidalPositionEmbeddings\n",
    "from diffusion_edf.equivariant_score_model import ScoreModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'\n",
    "eval = True\n",
    "compile = False\n",
    "\n",
    "model_configs_dir = 'configs/test/model_configs.yaml'\n",
    "train_configs_dir = 'configs/test/train_configs.yaml'\n",
    "task_configs_dir = 'configs/test/task_configs.yaml'\n",
    "\n",
    "with open(model_configs_dir) as file:\n",
    "    model_configs = yaml.load(file, Loader=yaml.FullLoader)\n",
    "with open(train_configs_dir) as file:\n",
    "    train_configs = yaml.load(file, Loader=yaml.FullLoader)\n",
    "with open(task_configs_dir) as file:\n",
    "    task_configs = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "train_configs['preprocess_config'].append({\n",
    "    'name': 'Rescale',\n",
    "    'kwargs': {'rescale_factor': 1/task_configs['unit_length']}\n",
    "})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load configs, preprocessor, and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_fn = []\n",
    "for proc in train_configs['preprocess_config']:\n",
    "    proc_fn.append(\n",
    "        getattr(preprocess, proc['name'])(**proc['kwargs'])\n",
    "    )\n",
    "proc_fn = Compose(proc_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "collate_fn = train_utils.get_collate_fn(task=train_configs['task_type'], proc_fn=proc_fn)\n",
    "trainset = DemoSeqDataset(dataset_dir=train_configs['dataset_dir'], annotation_file=train_configs['annotation_file'], device=device)\n",
    "train_dataloader = DataLoader(trainset, shuffle=True, collate_fn=collate_fn, batch_size=train_configs['n_batches'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ScoreModel: Initializing Score Head\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hw/anaconda3/envs/diff_edf/lib/python3.8/site-packages/torch/jit/_check.py:181: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\"The TorchScript type system doesn't support \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ScoreModel: Initializing Key Feature Extractor\n"
     ]
    }
   ],
   "source": [
    "score_model = ScoreModel(**model_configs, deterministic=True).to(device=device)\n",
    "if compile:\n",
    "    raise NotImplementedError\n",
    "if eval:\n",
    "    score_model = score_model.eval()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loop Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_T = 5\n",
    "\n",
    "for demo_batch in train_dataloader:\n",
    "    B = len(demo_batch)\n",
    "    scene_pcd, grasp_pcd, target_poses = train_utils.flatten_batch(demo_batch=demo_batch) # target_poses: (Nbatch, Ngrasps, 7)\n",
    "\n",
    "    q = transforms.random_quaternions(N_T, device=device)\n",
    "    x = torch.randn(N_T,3,device=device)\n",
    "    Ts = torch.cat([q, x], dim=-1)\n",
    "    time = torch.rand(N_T,device=device)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_out_multiscale = score_model.key_feature_extractor(pcd=scene_pcd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_points = detach_featured_points(scene_out_multiscale[-1])\n",
    "if score_model.score_head.key_tensor_field.irreps_query is not None:\n",
    "    query_points = set_featured_points_attribute(query_points, f=score_model.score_head.key_tensor_field.irreps_query.randn(len(query_points.x), -1, device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if score_model.score_head.key_tensor_field.context_emb_dim is None:\n",
    "    context_emb = None\n",
    "else:\n",
    "    context_emb = torch.randn(len(query_points.x), score_model.score_head.key_tensor_field.context_emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = score_model.score_head.key_tensor_field(query_points=query_points, input_points_multiscale=scene_out_multiscale, context_emb=context_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(False, device='cuda:0')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.f.isnan().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sdfa' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sdfa\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sdfa' is not defined"
     ]
    }
   ],
   "source": [
    "sdfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_pcd.x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdaf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    (ang_score, lin_score), (scene_out, grasp_out) = score_model(Ts=Ts, time=time,\n",
    "                                                                 key_pcd=scene_pcd, \n",
    "                                                                 query_pcd=grasp_pcd, \n",
    "                                                                 extract_features = True,\n",
    "                                                                 debug = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pcd = PointCloud(points=scene_out.x.detach().cpu(), colors=scene_out.w.detach().cpu(), cmap='magma')\n",
    "# pcd.show(point_size=3., width=800, height=800)\n",
    "\n",
    "# pcd = PointCloud(points=grasp_out.x.detach().cpu(), colors=grasp_out.w.detach().cpu(), cmap='magma')\n",
    "# pcd.show(point_size=8., width=800, height=800)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rotate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusion_edf.transforms import quaternion_apply, random_quaternions\n",
    "from diffusion_edf.gnn_data import TransformPcd\n",
    "\n",
    "transform_input = torch.jit.script(TransformPcd(irreps=\"3x0e\").to(device=device))\n",
    "transform_key = torch.jit.script(TransformPcd(irreps=model_configs['key_kwargs']['feature_extractor_configs']['irreps_output']).to(device=device))\n",
    "transform_query = torch.jit.script(TransformPcd(irreps=model_configs['query_kwargs']['feature_extractor_configs']['irreps_output']).to(device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rot1 = transforms.random_quaternions(1, device=device)\n",
    "trans1 = torch.randn(1,3,device=device)\n",
    "# rot1 = torch.tensor([1., 0., 0., 0.], device=device).expand(N_T,-1)\n",
    "# trans1 = torch.zeros(N_T,3, device=device)\n",
    "Ts_1 = torch.cat([rot1, trans1], dim=-1)\n",
    "scene_pcd_rot: FeaturedPoints = flatten_featured_points(transform_input(scene_pcd, Ts_1))\n",
    "scene_out_post_rot = transform_key(scene_out, Ts=Ts_1)\n",
    "\n",
    "rot2 = transforms.random_quaternions(1, device=device)\n",
    "trans2 = torch.randn(1,3,device=device)\n",
    "# rot2 = torch.tensor([1., 0., 0., 0.], device=device).expand(N_T,-1)\n",
    "# trans2 = torch.zeros(N_T,3, device=device)\n",
    "Ts_2 = torch.cat([rot2, trans2], dim=-1)\n",
    "\n",
    "grasp_pcd_rot: FeaturedPoints = flatten_featured_points(transform_input(grasp_pcd, Ts_2))\n",
    "grasp_out_post_rot = transform_key(grasp_out, Ts=Ts_2)\n",
    "\n",
    "lin_score_post_rot = quaternion_apply(rot2, lin_score)\n",
    "ang_score_post_rot = quaternion_apply(rot2, ang_score) + torch.cross(trans2, lin_score_post_rot)\n",
    "\n",
    "q_rot, x_rot = q, x\n",
    "q_rot = transforms.quaternion_multiply(rot1, q_rot)\n",
    "x_rot = transforms.quaternion_apply(rot1, x_rot) + trans1\n",
    "q_rot = transforms.quaternion_multiply(q_rot, transforms.quaternion_invert(rot2))\n",
    "x_rot = x_rot - transforms.quaternion_apply(q_rot, trans2)\n",
    "\n",
    "Ts_rot = torch.cat([q_rot, x_rot], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    (ang_score_pre_rot, lin_score_pre_rot), (scene_out_pre_rot, grasp_out_pre_rot) = score_model(Ts=Ts_rot, time=time,\n",
    "                                                                                                 key_pcd=scene_pcd_rot, \n",
    "                                                                                                 query_pcd=grasp_pcd_rot, \n",
    "                                                                                                 extract_features = True,\n",
    "                                                                                                 debug = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Scene feature extractor equivariance ===\")\n",
    "\n",
    "isclose = torch.isclose(scene_out_pre_rot.x, scene_out_post_rot.x, atol=0.01, rtol=0.01)\n",
    "print(f\"Position Equivariance ratio: {(isclose.sum() / len(scene_out_pre_rot.x.view(-1))).item()}\") # Slight non-equivariance comes from FPS downsampling algorithm.\n",
    "isclose = torch.isclose(scene_out_pre_rot.f, scene_out_post_rot.f, atol=0.001, rtol=0.001)\n",
    "print(f\"Feature Equivariance ratio: {(isclose.sum() / len(scene_out_pre_rot.f.view(-1))).item()}\") # Slight non-equivariance comes from FPS downsampling algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Grasp feature extractor equivariance ===\")\n",
    "\n",
    "isclose = torch.isclose(grasp_out_pre_rot.x, grasp_out_post_rot.x, atol=0.01, rtol=0.01)\n",
    "print(f\"Position Equivariance ratio: {(isclose.sum() / len(grasp_out_pre_rot.x.view(-1))).item()}\") # Slight non-equivariance comes from FPS downsampling algorithm.\n",
    "isclose = torch.isclose(grasp_out_pre_rot.f, grasp_out_post_rot.f, atol=0.001, rtol=0.001)\n",
    "print(f\"Feature Equivariance ratio: {(isclose.sum() / len(grasp_out_pre_rot.f.view(-1))).item()}\") # Slight non-equivariance comes from FPS downsampling algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Score equivariance ===\")\n",
    "\n",
    "isclose = torch.isclose(ang_score_pre_rot, ang_score_post_rot, atol=0.01, rtol=0.01)\n",
    "print(f\"Angular Score Equivariance ratio: {(isclose.sum() / len(ang_score_pre_rot.view(-1))).item()}\") # Slight non-equivariance comes from FPS downsampling algorithm.\n",
    "isclose = torch.isclose(lin_score_pre_rot, lin_score_post_rot,  atol=0.001, rtol=0.001)\n",
    "print(f\"Linear Score Equivariance ratio: {(isclose.sum() / len(lin_score_pre_rot.view(-1))).item()}\") # Slight non-equivariance comes from FPS downsampling algorithm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diff_edf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "79a0085b6cf04e1cff261ad12d41cff4e1530d9e68d1f8fc6bd159a2915452c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
