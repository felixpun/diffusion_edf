{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_JIT_USE_NNC_NOT_NVFUSER\"] = \"1\"\n",
    "from typing import List, Tuple, Optional, Union, Iterable\n",
    "\n",
    "from beartype import beartype\n",
    "import datetime\n",
    "import plotly.graph_objects as go\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Compose\n",
    "from open3d.visualization.tensorboard_plugin import summary\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from e3nn import o3\n",
    "\n",
    "from diffusion_edf.data import DemoSeqDataset, DemoSequence, TargetPoseDemo, PointCloud, SE3\n",
    "from diffusion_edf.gnn_data import FeaturedPoints, merge_featured_points, GraphEdge, flatten_featured_points, set_featured_points_attribute, _featured_points_repr\n",
    "from diffusion_edf import train_utils\n",
    "from diffusion_edf import preprocess\n",
    "from diffusion_edf import transforms\n",
    "from diffusion_edf.feature_extractor import UnetFeatureExtractor\n",
    "from diffusion_edf.tensor_field import TensorField\n",
    "from diffusion_edf.radial_func import SinusoidalPositionEmbeddings\n",
    "from diffusion_edf.equivariant_score_model import ScoreModel\n",
    "from diffusion_edf.utils import sample_reference_points\n",
    "from diffusion_edf.dist import diffuse_isotropic_se3, adjoint_inv_tr_isotropic_se3_score, diffuse_isotropic_se3_batched\n",
    "\n",
    "torch.set_printoptions(precision=4, sci_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'\n",
    "# eval = True\n",
    "compile = False\n",
    "\n",
    "model_configs_dir = 'configs/test/model_configs.yaml'\n",
    "train_configs_dir = 'configs/test/train_configs.yaml'\n",
    "task_configs_dir = 'configs/test/task_configs.yaml'\n",
    "\n",
    "with open(model_configs_dir) as file:\n",
    "    model_configs = yaml.load(file, Loader=yaml.FullLoader)\n",
    "with open(train_configs_dir) as file:\n",
    "    train_configs = yaml.load(file, Loader=yaml.FullLoader)\n",
    "with open(task_configs_dir) as file:\n",
    "    task_configs = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "train_configs['preprocess_config'].append({\n",
    "    'name': 'Rescale',\n",
    "    'kwargs': {'rescale_factor': 1/task_configs['unit_length']}\n",
    "})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_fn = []\n",
    "for proc in train_configs['preprocess_config']:\n",
    "    proc_fn.append(\n",
    "        getattr(preprocess, proc['name'])(**proc['kwargs'])\n",
    "    )\n",
    "proc_fn = Compose(proc_fn)\n",
    "\n",
    "\n",
    "collate_fn = train_utils.get_collate_fn(task=train_configs['task_type'], proc_fn=proc_fn)\n",
    "trainset = DemoSeqDataset(dataset_dir=train_configs['dataset_dir'], annotation_file=train_configs['annotation_file'], device=device)\n",
    "train_dataloader = DataLoader(trainset, shuffle=True, collate_fn=collate_fn, batch_size=train_configs['n_batches'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ScoreModel: Initializing Score Head\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hw/anaconda3/envs/diff_edf/lib/python3.8/site-packages/torch/jit/_check.py:181: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\"The TorchScript type system doesn't support \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ScoreModel: Initializing Key Feature Extractor\n",
      "ScoreModel: Initializing Query Feature Extractor\n"
     ]
    }
   ],
   "source": [
    "score_model = ScoreModel(**model_configs, deterministic=False).to(device=device)\n",
    "if compile:\n",
    "    raise NotImplementedError\n",
    "# if eval:\n",
    "#     score_model = score_model.eval()\n",
    "\n",
    "optimizer = torch.optim.Adam(list(score_model.parameters()), **train_configs['optimizer_kwargs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_log_dir: Optional[str] = None\n",
    "#resume_log_dir: Optional[str] = 'runs/2023_04_21_00-34-42'\n",
    "resume_checkpoint_dir: Optional[str] = None\n",
    "if resume_log_dir is not None:\n",
    "    if resume_checkpoint_dir is None:\n",
    "        resume_checkpoint_dir = sorted(os.listdir(os.path.join(resume_log_dir, f'checkpoint')), key= lambda f:int(f.rstrip('.pt')))[-1]\n",
    "    resume_training = True\n",
    "    if input(f\"Enter 'y' if you want to resume training from checkpoint: {os.path.join(resume_log_dir, f'checkpoint', resume_checkpoint_dir)}\") == 'y':\n",
    "        pass\n",
    "    else:\n",
    "        raise ValueError()\n",
    "else:\n",
    "    resume_training = False\n",
    "    resume_log_dir = os.path.join(train_configs['log_dir_root'], f\"{datetime.datetime.now().strftime('%Y_%m_%d_%H-%M-%S')}\")\n",
    "\n",
    "writer = SummaryWriter(log_dir=resume_log_dir)\n",
    "log_dir = writer.log_dir\n",
    "\n",
    "if not os.path.exists(os.path.join(log_dir, f'checkpoint')):\n",
    "    os.mkdir(os.path.join(log_dir, f'checkpoint'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if resume_training:\n",
    "    checkpoint = torch.load(os.path.join(log_dir, f'checkpoint', resume_checkpoint_dir))\n",
    "    score_model.load_state_dict(checkpoint['score_model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    steps = checkpoint['steps']\n",
    "    print(f\"resume training from checkpoint: {os.path.join(log_dir, f'checkpoint', resume_checkpoint_dir)}\")\n",
    "    epoch = epoch + 1\n",
    "else:\n",
    "    epoch = 0\n",
    "    steps = 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = train_configs['max_epochs']\n",
    "n_epochs_per_checkpoint = train_configs['n_epochs_per_checkpoint']\n",
    "n_samples_x_ref = train_configs['n_samples_x_ref']\n",
    "contact_radius = train_configs['contact_radius']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch: 0) Successfully saved logs to: runs/2023_05_25_13-47-46\n",
      "(Epoch: 20) Successfully saved logs to: runs/2023_05_25_13-47-46\n",
      "(Epoch: 40) Successfully saved logs to: runs/2023_05_25_13-47-46\n",
      "(Epoch: 60) Successfully saved logs to: runs/2023_05_25_13-47-46\n",
      "(Epoch: 80) Successfully saved logs to: runs/2023_05_25_13-47-46\n",
      "(Epoch: 100) Successfully saved logs to: runs/2023_05_25_13-47-46\n",
      "(Epoch: 120) Successfully saved logs to: runs/2023_05_25_13-47-46\n",
      "(Epoch: 140) Successfully saved logs to: runs/2023_05_25_13-47-46\n",
      "(Epoch: 160) Successfully saved logs to: runs/2023_05_25_13-47-46\n",
      "(Epoch: 180) Successfully saved logs to: runs/2023_05_25_13-47-46\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epoch, max_epochs+1):\n",
    "    for demo_batch in train_dataloader:\n",
    "        B = len(demo_batch)\n",
    "        assert B == 1, \"Batch training is not supported yet.\"\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        scene_input, grasp_input, T_target = train_utils.flatten_batch(demo_batch=demo_batch) # T_target: (Nbatch, Ngrasps, 7)\n",
    "        T_target = T_target.squeeze(0) # (B=1, N_poses=1, 7) -> (1,7) \n",
    "\n",
    "        min_time = torch.tensor([1e-3], dtype=T_target.dtype, device=T_target.device)\n",
    "        max_time = torch.tensor([1.0], dtype=T_target.dtype, device=T_target.device)\n",
    "        time_in = (min_time/max_time + torch.rand(1, dtype=T_target.dtype, device=T_target.device) * (1-min_time/max_time))*max_time   # Shape: (1,)\n",
    "        #time_in = torch.exp(torch.rand_like(max_time) * (torch.log(max_time)-torch.log(min_time)) + torch.log(min_time))              # Shape: (1,)\n",
    "\n",
    "        eps = time_in / 2   # Shape: (1,)\n",
    "        std = torch.sqrt(time_in) * score_model.lin_mult   # Shape: (1,)\n",
    "        x_ref, n_neighbors = sample_reference_points(\n",
    "            src_points = PointCloud(points=scene_input.x, colors=scene_input.f).transformed(\n",
    "                                    SE3(T_target).inv(), squeeze=True\n",
    "                                    ).points, \n",
    "            dst_points = grasp_input.x, \n",
    "            r=contact_radius, \n",
    "            n_samples=n_samples_x_ref\n",
    "        )\n",
    "        # T, delta_T, (gt_ang_score, gt_lin_score), (gt_ang_score_ref, gt_lin_score_ref) = diffuse_isotropic_se3(T0 = T_target, eps=eps, std=std, x_ref=x_ref, double_precision=True)\n",
    "        T, delta_T, (gt_ang_score, gt_lin_score), (gt_ang_score_ref, gt_lin_score_ref) = diffuse_isotropic_se3_batched(T0 = T_target, eps=eps, std=std, x_ref=x_ref, double_precision=True)\n",
    "        T, delta_T, (gt_ang_score, gt_lin_score), (gt_ang_score_ref, gt_lin_score_ref) = T.squeeze(-2), delta_T.squeeze(-2), (gt_ang_score.squeeze(-2), gt_lin_score.squeeze(-2)), (gt_ang_score_ref.squeeze(-2), gt_lin_score_ref.squeeze(-2))\n",
    "        # T: (nT, 7) || delta_T: (nT, 7) || gt_*_score_*: (nT, 3) ||\n",
    "        # Note that nT = n_samples_x_ref * nT_target  ||   nT_target = 1\n",
    "\n",
    "\n",
    "        time_in_ = time_in.repeat(len(T))\n",
    "        (ang_score, lin_score), (scene_out, grasp_out) = score_model(Ts=T, time=time_in_,\n",
    "                                                                     key_pcd=scene_input, \n",
    "                                                                     query_pcd=grasp_input, \n",
    "                                                                     extract_features = True,\n",
    "                                                                     debug = True)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            ang_score_ref, lin_score_ref = adjoint_inv_tr_isotropic_se3_score(x_ref=-x_ref, ang_score=ang_score, lin_score=lin_score)\n",
    "        target_ang_score = gt_ang_score * torch.sqrt(time_in)\n",
    "        target_lin_score = gt_lin_score * torch.sqrt(time_in)\n",
    "        target_ang_score_ref = gt_ang_score_ref * torch.sqrt(time_in)\n",
    "        target_lin_score_ref = gt_lin_score_ref * torch.sqrt(time_in)\n",
    "\n",
    "\n",
    "        ang_score_diff = target_ang_score - ang_score\n",
    "        lin_score_diff = target_lin_score - lin_score\n",
    "        # ang_loss = torch.norm(ang_score_diff, dim=-1).mean(dim=-1)\n",
    "        # lin_loss = torch.norm(lin_score_diff * lin_mult, dim=-1).mean(dim=-1)\n",
    "        ang_loss = torch.sum(torch.square(ang_score_diff), dim=-1).mean(dim=-1)\n",
    "        lin_loss = torch.sum(torch.square(lin_score_diff * score_model.lin_mult), dim=-1).mean(dim=-1)\n",
    "        loss = ang_loss + lin_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            writer.add_scalar(tag=\"Loss/train\", scalar_value=loss.item(), global_step=steps)\n",
    "            writer.add_scalar(tag=\"Loss/angular\", scalar_value=ang_loss.item(), global_step=steps)\n",
    "            writer.add_scalar(tag=\"Loss/linear\", scalar_value=lin_loss.item(), global_step=steps)\n",
    "\n",
    "            target_norm_ang, target_norm_lin = torch.norm(target_ang_score.detach(), dim=-1), torch.norm(target_lin_score.detach(), dim=-1) # Shape: (Nbatch, ), (Nbatch, )\n",
    "            score_norm_ang, score_norm_lin = torch.norm(ang_score.detach(), dim=-1), torch.norm(lin_score.detach(), dim=-1)         # Shape: (Nbatch, ), (Nbatch, )\n",
    "            writer.add_scalar(tag=\"norm/target_ang\", scalar_value=target_norm_ang.mean(dim=-1).item(), global_step=steps)\n",
    "            writer.add_scalar(tag=\"norm/target_lin\", scalar_value=target_norm_lin.mean(dim=-1).item(), global_step=steps)\n",
    "            writer.add_scalar(tag=\"norm/inferred_ang\", scalar_value=score_norm_ang.mean(dim=-1).item(), global_step=steps)\n",
    "            writer.add_scalar(tag=\"norm/inferred_lin\", scalar_value=score_norm_lin.mean(dim=-1).item(), global_step=steps)\n",
    "\n",
    "            target_norm_ang_ref, target_norm_lin_ref = torch.norm(target_ang_score_ref.detach(), dim=-1), torch.norm(target_lin_score_ref.detach(), dim=-1) # Shape: (Nbatch, ), (Nbatch, )\n",
    "            score_norm_ang_ref, score_norm_lin_ref = torch.norm(ang_score_ref.detach(), dim=-1), torch.norm(lin_score_ref.detach(), dim=-1)         # Shape: (Nbatch, ), (Nbatch, )\n",
    "            writer.add_scalar(tag=\"norm_ref/target_ang\", scalar_value=target_norm_ang_ref.mean(dim=-1).item(), global_step=steps)\n",
    "            writer.add_scalar(tag=\"norm_ref/target_lin\", scalar_value=target_norm_lin_ref.mean(dim=-1).item(), global_step=steps)\n",
    "            writer.add_scalar(tag=\"norm_ref/inferred_ang\", scalar_value=score_norm_ang_ref.mean(dim=-1).item(), global_step=steps)\n",
    "            writer.add_scalar(tag=\"norm_ref/inferred_lin\", scalar_value=score_norm_lin_ref.mean(dim=-1).item(), global_step=steps)\n",
    "\n",
    "            dp_align_ang = torch.einsum('...i,...i->...', ang_score.detach(), target_ang_score.detach()) # Shape: (Nbatch, )\n",
    "            dp_align_lin = torch.einsum('...i,...i->...', lin_score.detach(), target_lin_score.detach()) # Shape: (Nbatch, )\n",
    "            dp_align_ang_normalized = dp_align_ang / target_norm_ang / score_norm_ang # Shape: (Nbatch, )\n",
    "            dp_align_lin_normalized = dp_align_lin / target_norm_lin / score_norm_lin # Shape: (Nbatch, )\n",
    "            writer.add_scalar(tag=\"alignment/unnormalized/ang\", scalar_value=dp_align_ang.mean(dim=-1).item(), global_step=steps)\n",
    "            writer.add_scalar(tag=\"alignment/unnormalized/lin\", scalar_value=dp_align_lin.mean(dim=-1).item(), global_step=steps)\n",
    "            writer.add_scalar(tag=\"alignment/normalized/ang\", scalar_value=dp_align_ang_normalized.mean(dim=-1).item(), global_step=steps)\n",
    "            writer.add_scalar(tag=\"alignment/normalized/lin\", scalar_value=dp_align_lin_normalized.mean(dim=-1).item(), global_step=steps)\n",
    "\n",
    "        steps += 1\n",
    "\n",
    "    if epoch % n_epochs_per_checkpoint == 0:\n",
    "        torch.save({'epoch': epoch,\n",
    "                    'steps': steps,\n",
    "                    'score_model_state_dict': score_model.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    }, os.path.join(log_dir, f'checkpoint/{epoch}.pt'))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            scene_pcd = PointCloud(points=scene_input.x, colors=scene_input.f)\n",
    "            grasp_pcd = PointCloud(points=grasp_input.x, colors=grasp_input.f)\n",
    "            target_pose_pcd = PointCloud.merge(\n",
    "                scene_pcd,\n",
    "                grasp_pcd.transformed(SE3(T_target), squeeze=True),\n",
    "            )\n",
    "            diffused_pose_pcd = PointCloud.merge(\n",
    "                scene_pcd,\n",
    "                grasp_pcd.transformed(SE3(T))[0],\n",
    "            )\n",
    "            \n",
    "            ##### Query Summary #####\n",
    "            query_weight, query_points, query_point_batch = grasp_out.w.detach(), grasp_out.x.detach(), grasp_out.b.detach(), \n",
    "            batch_vis_idx = (query_point_batch == 0).nonzero().squeeze(-1)\n",
    "            query_weight, query_points = query_weight[batch_vis_idx], query_points[batch_vis_idx]\n",
    "\n",
    "            N_repeat = 500\n",
    "            query_points_colors = torch.tensor([0.01, 1., 1.], device=query_weight.device, dtype=query_weight.dtype).expand(N_repeat, 1, 3) * query_weight[None, :, None]\n",
    "            r_query_ball = 0.5\n",
    "\n",
    "            ball = torch.randn(N_repeat,1,3, device=query_points.device, dtype=query_points.dtype)\n",
    "            ball = ball/ball.norm(dim=-1, keepdim=True) * r_query_ball\n",
    "            query_points = (query_points + ball).reshape(-1,3)\n",
    "            query_points_colors = query_points_colors.reshape(-1,3)\n",
    "\n",
    "            #########################\n",
    "            scene_attn_pcd = PointCloud(points=scene_out.x.detach().cpu(), \n",
    "                                        colors=scene_out.w.detach().cpu(),\n",
    "                                        cmap='magma')\n",
    "        \n",
    "        writer.add_3d(\n",
    "            tag = \"Target Pose\",\n",
    "            data = {\n",
    "                \"vertex_positions\": target_pose_pcd.points.cpu(),\n",
    "                \"vertex_colors\": target_pose_pcd.colors.cpu(),  # (N, 3)\n",
    "            },\n",
    "            step=epoch//n_epochs_per_checkpoint,\n",
    "        )\n",
    "\n",
    "        writer.add_3d(\n",
    "            tag = \"Diffused Pose\",\n",
    "            data = {\n",
    "                \"vertex_positions\": diffused_pose_pcd.points.cpu(),\n",
    "                \"vertex_colors\": diffused_pose_pcd.colors.cpu(),  # (N, 3)\n",
    "            },\n",
    "            step=epoch//n_epochs_per_checkpoint,\n",
    "            description=f\"Diffuse time: {time_in.item()} || eps: {eps.item()} || std: {std.item()}\",\n",
    "        )\n",
    "\n",
    "        writer.add_3d(\n",
    "            tag = \"Grasp\",\n",
    "            data = {\n",
    "                \"vertex_positions\": grasp_pcd.points.cpu(),\n",
    "                \"vertex_colors\": grasp_pcd.colors.cpu(),  # (N, 3)\n",
    "            },\n",
    "            step=epoch//n_epochs_per_checkpoint,\n",
    "        )\n",
    "\n",
    "        writer.add_3d(\n",
    "            tag = \"Query points\",\n",
    "            data = {\n",
    "                # \"vertex_positions\": query_points.repeat(max(int(1000//len(query_points)),1),1).cpu(),      # There is a bug with too small number of points so repeat\n",
    "                # \"vertex_colors\": query_points_colors.repeat(max(int(1000//len(query_points)),1),1).cpu(),  # (N, 3)\n",
    "                \"vertex_positions\": query_points.cpu(),      # There is a bug with too small number of points so repeat\n",
    "                \"vertex_colors\": query_points_colors.cpu(),  # (N, 3)\n",
    "            },\n",
    "            step=epoch//n_epochs_per_checkpoint,\n",
    "        )\n",
    "\n",
    "        writer.add_3d(\n",
    "            tag = \"Scene Attention\",\n",
    "            data = {\n",
    "                \"vertex_positions\": scene_attn_pcd.points.cpu(),\n",
    "                \"vertex_colors\": scene_attn_pcd.colors.cpu(),  # (N, 3)\n",
    "            },\n",
    "            step=epoch//n_epochs_per_checkpoint,\n",
    "        )\n",
    "\n",
    "        \n",
    "        print(f\"(Epoch: {epoch}) Successfully saved logs to: {log_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_model.score_head.key_tensor_field.time_emb_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diff_edf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "79a0085b6cf04e1cff261ad12d41cff4e1530d9e68d1f8fc6bd159a2915452c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
