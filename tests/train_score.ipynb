{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"PYTORCH_JIT_USE_NNC_NOT_NVFUSER\"] = \"1\"\n",
    "from typing import List, Tuple, Optional, Union, Iterable\n",
    "import math\n",
    "\n",
    "from beartype import beartype\n",
    "import datetime\n",
    "import plotly.graph_objects as go\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Compose\n",
    "from e3nn import o3\n",
    "\n",
    "from diffusion_edf.data import DemoSeqDataset, DemoSequence, TargetPoseDemo, PointCloud, SE3\n",
    "from diffusion_edf.gnn_data import FeaturedPoints, merge_featured_points, GraphEdge, flatten_featured_points, set_featured_points_attribute, _featured_points_repr\n",
    "from diffusion_edf import train_utils\n",
    "from diffusion_edf import transforms\n",
    "from diffusion_edf.utils import sample_reference_points\n",
    "from diffusion_edf.dist import diffuse_isotropic_se3, adjoint_inv_tr_isotropic_se3_score, diffuse_isotropic_se3_batched\n",
    "from diffusion_edf.point_attentive_score_model import PointAttentiveScoreModel\n",
    "from diffusion_edf.trainer import DiffusionEdfTrainer\n",
    "\n",
    "torch.set_printoptions(precision=4, sci_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ScoreModel: Initializing Key Model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hw/anaconda3/envs/diff_edf/lib/python3.8/site-packages/torch/jit/_check.py:181: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\"The TorchScript type system doesn't support \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ScoreModel: Initializing Query Model\n",
      "ScoreModel: Initializing Score Head\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configs_root_dir = 'configs/pick_point_attn'\n",
    "train_configs_file = 'train_configs.yaml'\n",
    "task_configs_file = 'task_configs.yaml'\n",
    "trainer = DiffusionEdfTrainer(configs_root_dir=configs_root_dir,\n",
    "                              train_configs_file=train_configs_file,\n",
    "                              task_configs_file=task_configs_file)\n",
    "trainer.init(log_name = trainer.get_current_time(postfix=\"MultiphaseTraining\"),\n",
    "             resume_training = False,)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch: 0) Successfully saved logs to: runs/2023_05_31_00-31-39_MultiphaseTraining\n",
      "(Epoch: 20) Successfully saved logs to: runs/2023_05_31_00-31-39_MultiphaseTraining\n",
      "(Epoch: 40) Successfully saved logs to: runs/2023_05_31_00-31-39_MultiphaseTraining\n",
      "(Epoch: 60) Successfully saved logs to: runs/2023_05_31_00-31-39_MultiphaseTraining\n",
      "(Epoch: 80) Successfully saved logs to: runs/2023_05_31_00-31-39_MultiphaseTraining\n",
      "(Epoch: 100) Successfully saved logs to: runs/2023_05_31_00-31-39_MultiphaseTraining\n",
      "(Epoch: 120) Successfully saved logs to: runs/2023_05_31_00-31-39_MultiphaseTraining\n",
      "(Epoch: 140) Successfully saved logs to: runs/2023_05_31_00-31-39_MultiphaseTraining\n",
      "(Epoch: 160) Successfully saved logs to: runs/2023_05_31_00-31-39_MultiphaseTraining\n",
      "(Epoch: 180) Successfully saved logs to: runs/2023_05_31_00-31-39_MultiphaseTraining\n",
      "(Epoch: 200) Successfully saved logs to: runs/2023_05_31_00-31-39_MultiphaseTraining\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(trainer.epoch, trainer.max_epochs+1):\n",
    "    for demo_batch in trainer.trainloader:\n",
    "        B = len(demo_batch)\n",
    "        assert B == 1, \"Batch training is not supported yet.\"\n",
    "\n",
    "        trainer.optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        scene_input, grasp_input, T_target = train_utils.flatten_batch(demo_batch=demo_batch) # T_target: (Nbatch, Ngrasps, 7)\n",
    "        T_target = T_target.squeeze(0) # (B=1, N_poses=1, 7) -> (1,7) \n",
    "\n",
    "        ########################################## Augmentation #########################################\n",
    "        if trainer.t_augment is not None:\n",
    "            x_ref, _ = train_utils.transform_and_sample_reference_points(T_target=T_target,\n",
    "                                                                         scene_points=scene_input,\n",
    "                                                                         grasp_points=grasp_input,\n",
    "                                                                         contact_radius=trainer.contact_radius,\n",
    "                                                                         n_samples_x_ref=1)\n",
    "            T_target, _, __, ___, ____ = train_utils.diffuse_T_target(T_target=T_target, \n",
    "                                                                      x_ref=x_ref, \n",
    "                                                                      time=torch.tensor([trainer.t_augment], device=T_target.device), \n",
    "                                                                      lin_mult=trainer.score_model.lin_mult,\n",
    "                                                                      ang_mult=trainer.score_model.ang_mult)\n",
    "        ##################################################################################################\n",
    "\n",
    "        # time_schedule = trainer.diffusion_schedules[torch.randint(low=0, high=trainer.n_schedules, size=(1,)).item()]\n",
    "        # time = train_utils.random_time(min_time=time_schedule[1], max_time=time_schedule[0], device=T_target.device) # Shape: (1,)\n",
    "        time_in = torch.empty(0, device=trainer.device)\n",
    "        T = torch.empty(0,7, device=trainer.device)\n",
    "        gt_ang_score, gt_lin_score = torch.empty(0,3, device=trainer.device), torch.empty(0,3, device=trainer.device)\n",
    "        gt_ang_score_ref, gt_lin_score_ref = torch.empty(0,3, device=trainer.device), torch.empty(0,3, device=trainer.device)\n",
    "        \n",
    "        for time_schedule in trainer.diffusion_schedules:\n",
    "            time_ = train_utils.random_time(min_time=time_schedule[1], max_time=time_schedule[0], device=T_target.device) # Shape: (1,)\n",
    "            \n",
    "            x_ref_, n_neighbors_ = train_utils.transform_and_sample_reference_points(T_target=T_target,\n",
    "                                                                                scene_points=scene_input,\n",
    "                                                                                grasp_points=grasp_input,\n",
    "                                                                                contact_radius=trainer.contact_radius,\n",
    "                                                                                n_samples_x_ref=trainer.n_samples_x_ref)\n",
    "            T_, delta_T_, time_in_, gt_score_, gt_score_ref_ = train_utils.diffuse_T_target(T_target=T_target, \n",
    "                                                                                    x_ref=x_ref_, \n",
    "                                                                                    time=time_, \n",
    "                                                                                    lin_mult=trainer.score_model.lin_mult,\n",
    "                                                                                    ang_mult=trainer.score_model.ang_mult)\n",
    "            \n",
    "            \n",
    "            (gt_ang_score_, gt_lin_score_), (gt_ang_score_ref_, gt_lin_score_ref_) = gt_score_, gt_score_ref_\n",
    "            T = torch.cat([T, T_], dim=0)\n",
    "            time_in = torch.cat([time_in, time_in_], dim=0)\n",
    "            gt_ang_score = torch.cat([gt_ang_score, gt_ang_score_], dim=0)\n",
    "            gt_lin_score = torch.cat([gt_lin_score, gt_lin_score_], dim=0)\n",
    "            gt_ang_score_ref = torch.cat([gt_ang_score_ref, gt_ang_score_ref_], dim=0)\n",
    "            gt_lin_score_ref = torch.cat([gt_lin_score_ref, gt_lin_score_ref_], dim=0)\n",
    "\n",
    "        loss, fp_info, tensor_info, statistics = trainer.score_model.get_train_loss(Ts=T, time=time_in, key_pcd=scene_input, query_pcd=grasp_input,\n",
    "                                                                                    target_ang_score=gt_ang_score, target_lin_score=gt_lin_score)\n",
    "        scene_out: FeaturedPoints = fp_info['key_fp']\n",
    "        grasp_out: FeaturedPoints = fp_info['query_fp']\n",
    "\n",
    "        loss.backward()\n",
    "        trainer.optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for tag, scalar_value in statistics.items():\n",
    "                trainer.logger.add_scalar(tag=tag, scalar_value=scalar_value, global_step=trainer.steps)\n",
    "        trainer.steps += 1\n",
    "\n",
    "    if trainer.epoch % trainer.n_epochs_per_checkpoint == 0:\n",
    "        with torch.no_grad():\n",
    "            scene_pcd = PointCloud(points=scene_input.x, colors=scene_input.f)\n",
    "            grasp_pcd = PointCloud(points=grasp_input.x, colors=grasp_input.f)\n",
    "            target_pose_pcd = PointCloud.merge(\n",
    "                scene_pcd,\n",
    "                grasp_pcd.transformed(SE3(T_target), squeeze=True),\n",
    "            )\n",
    "            diffused_pose_pcd = PointCloud.merge(\n",
    "                scene_pcd,\n",
    "                grasp_pcd.transformed(SE3(T))[0],\n",
    "            )\n",
    "            scene_attn_pcd = PointCloud(points=scene_out.x.detach().cpu(), \n",
    "                                        colors=scene_out.w.detach().cpu(),\n",
    "                                        cmap='magma')\n",
    "            grasp_attn_pcd = PointCloud(points=grasp_out.x.detach().cpu(), \n",
    "                                        colors=grasp_out.w.detach().cpu(),\n",
    "                                        cmap='magma')\n",
    "        \n",
    "            query_weight, query_points, query_point_batch = grasp_out.w.detach(), grasp_out.x.detach(), grasp_out.b.detach(), \n",
    "            batch_vis_idx = (query_point_batch == 0).nonzero().squeeze(-1)\n",
    "            query_weight, query_points = query_weight[batch_vis_idx], query_points[batch_vis_idx]\n",
    "\n",
    "            N_repeat = 500\n",
    "            query_points_colors = torch.tensor([0.01, 1., 1.], device=query_weight.device, dtype=query_weight.dtype).expand(N_repeat, 1, 3) * query_weight[None, :, None]\n",
    "            r_query_ball = 0.5\n",
    "\n",
    "            ball = torch.randn(N_repeat,1,3, device=query_points.device, dtype=query_points.dtype)\n",
    "            ball = ball/ball.norm(dim=-1, keepdim=True) * r_query_ball\n",
    "            query_points = (query_points + ball).reshape(-1,3)\n",
    "            query_points_colors = query_points_colors.reshape(-1,3)\n",
    "\n",
    "        trainer.logger.add_3d(\n",
    "            tag = \"Scene Attention\",\n",
    "            data = {\n",
    "                \"vertex_positions\": scene_attn_pcd.points.cpu(),\n",
    "                \"vertex_colors\": scene_attn_pcd.colors.cpu(),  # (N, 3)\n",
    "            },\n",
    "            step=trainer.epoch//trainer.n_epochs_per_checkpoint,\n",
    "        )\n",
    "\n",
    "        trainer.logger.add_3d(\n",
    "            tag = \"Grasp Attention\",\n",
    "            data = {\n",
    "                # \"vertex_positions\": query_points.repeat(max(int(1000//len(query_points)),1),1).cpu(),      # There is a bug with too small number of points so repeat\n",
    "                # \"vertex_colors\": query_points_colors.repeat(max(int(1000//len(query_points)),1),1).cpu(),  # (N, 3)\n",
    "                \"vertex_positions\": query_points.cpu(),      # There is a bug with too small number of points so repeat\n",
    "                \"vertex_colors\": query_points_colors.cpu(),  # (N, 3)\n",
    "            },\n",
    "            step=trainer.epoch//trainer.n_epochs_per_checkpoint,\n",
    "        )\n",
    "\n",
    "        trainer.logger.add_3d(\n",
    "            tag = \"Target Pose\",\n",
    "            data = {\n",
    "                \"vertex_positions\": target_pose_pcd.points.cpu(),\n",
    "                \"vertex_colors\": target_pose_pcd.colors.cpu(),  # (N, 3)\n",
    "            },\n",
    "            step=trainer.epoch//trainer.n_epochs_per_checkpoint,\n",
    "        )\n",
    "\n",
    "        trainer.logger.add_3d(\n",
    "            tag = \"Diffused Pose\",\n",
    "            data = {\n",
    "                \"vertex_positions\": diffused_pose_pcd.points.cpu(),\n",
    "                \"vertex_colors\": diffused_pose_pcd.colors.cpu(),  # (N, 3)\n",
    "            },\n",
    "            step=trainer.epoch//trainer.n_epochs_per_checkpoint,\n",
    "            #description=f\"Diffuse time: {time_in[0].item()} || eps: {eps.item()} || std: {std.item()}\",\n",
    "        )\n",
    "\n",
    "        trainer.logger.add_3d(\n",
    "            tag = \"Grasp\",\n",
    "            data = {\n",
    "                \"vertex_positions\": grasp_pcd.points.cpu(),\n",
    "                \"vertex_colors\": grasp_pcd.colors.cpu(),  # (N, 3)\n",
    "            },\n",
    "            step=trainer.epoch//trainer.n_epochs_per_checkpoint,\n",
    "        )\n",
    "\n",
    "        torch.save({'epoch': trainer.epoch,\n",
    "                    'steps': trainer.steps,\n",
    "                    'score_model_state_dict': trainer.score_model.state_dict(),\n",
    "                    'optimizer_state_dict': trainer.optimizer.state_dict(),\n",
    "                    }, os.path.join(trainer.log_dir, f'checkpoint/{trainer.epoch}.pt'))\n",
    "        \n",
    "        print(f\"(Epoch: {trainer.epoch}) Successfully saved logs to: {trainer.log_dir}\")\n",
    "    trainer.epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diff_edf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "79a0085b6cf04e1cff261ad12d41cff4e1530d9e68d1f8fc6bd159a2915452c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
