{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# os.environ[\"PYTORCH_JIT_USE_NNC_NOT_NVFUSER\"] = \"1\"\n",
    "\n",
    "from typing import List, Tuple, Optional, Union, Iterable, NamedTuple\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from diffusion_edf.data import DemoSeqDataset, DemoSequence, TargetPoseDemo, FeaturedPoints, merge_featured_points\n",
    "from diffusion_edf import preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "voxel_size = 0.01\n",
    "task = 'pick'\n",
    "\n",
    "def proc_fn(demo_batch: List[TargetPoseDemo]):\n",
    "    scene_pcd = []\n",
    "    grasp_pcd = []\n",
    "    target_poses = []\n",
    "    for b, demo in enumerate(demo_batch):\n",
    "        demo: TargetPoseDemo = preprocess.downsample(data=demo, voxel_size=voxel_size, coord_reduction='average')\n",
    "        scene_pcd.append(demo.scene_pc.to_featured_points(batch_idx=b))\n",
    "        grasp_pcd.append(demo.grasp_pc.to_featured_points(batch_idx=b))\n",
    "        target_poses.append(demo.target_poses.poses)\n",
    "\n",
    "    scene_pcd = merge_featured_points(scene_pcd) # Shape: x: (b*p, 3), f: (b*p, 3), b: (b*p, )   # b: N_batch, p: N_points_scene\n",
    "    grasp_pcd = merge_featured_points(grasp_pcd) # Shape: x: (b*p, 3), f: (b*p, 3), b: (b*p, )   # b: N_batch, p: N_points_grasp\n",
    "    target_poses = torch.stack(target_poses, dim=0) # Shape: (b, g, 4+3)                         # g: N_poses\n",
    "\n",
    "    return scene_pcd, grasp_pcd, target_poses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_batch = 3\n",
    "if task == 'pick':\n",
    "    def collate_fn(data_batch: List[DemoSequence]) -> List[TargetPoseDemo]:\n",
    "        return [demo_seq[0] for demo_seq in data_batch]\n",
    "elif task == 'place':\n",
    "    def collate_fn(data_batch: List[DemoSequence]) -> List[TargetPoseDemo]:\n",
    "        return [demo_seq[1] for demo_seq in data_batch]\n",
    "else:\n",
    "    raise ValueError(f\"Unknown task name: {task}\")\n",
    "\n",
    "trainset = DemoSeqDataset(dataset_dir=\"test_data\", annotation_file=\"data.yaml\", device=device)\n",
    "train_dataloader = DataLoader(trainset, shuffle=True, collate_fn=collate_fn, batch_size=n_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(train_dataloader))\n",
    "scene_pcd, grasp_pcd, target_poses = proc_fn(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sadasfddsffa' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sadasfddsffa\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sadasfddsffa' is not defined"
     ]
    }
   ],
   "source": [
    "sadasfddsffa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_len = 0.01\n",
    "scene_voxel_size = 0.01\n",
    "grasp_voxel_size = 0.01\n",
    "\n",
    "scene_voxel_size = scene_voxel_size / unit_len\n",
    "grasp_voxel_size = grasp_voxel_size / unit_len\n",
    "\n",
    "\n",
    "rescale_fn = Rescale(rescale_factor=1/unit_len)\n",
    "recover_scale_fn = Rescale(rescale_factor=unit_len)\n",
    "normalize_color_fn = NormalizeColor(color_mean = torch.tensor([0.5, 0.5, 0.5]), color_std = torch.tensor([0.5, 0.5, 0.5]))\n",
    "recover_color_fn = NormalizeColor(color_mean = -normalize_color_fn.color_mean / normalize_color_fn.color_std, color_std = 1 / normalize_color_fn.color_std)\n",
    "\n",
    "\n",
    "scene_proc_fn = Compose([rescale_fn,\n",
    "                         Downsample(voxel_size=scene_voxel_size, coord_reduction=\"average\"),\n",
    "                         normalize_color_fn])\n",
    "scene_unproc_fn = Compose([recover_color_fn, recover_scale_fn])\n",
    "grasp_proc_fn = Compose([rescale_fn,\n",
    "                         Downsample(voxel_size=grasp_voxel_size, coord_reduction=\"average\"),\n",
    "                         normalize_color_fn])\n",
    "grasp_unproc_fn = Compose([recover_color_fn, recover_scale_fn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "device = 'cuda:0'\n",
    "eval = True\n",
    "\n",
    "irreps_input = o3.Irreps('3x0e')\n",
    "irreps_node_embedding = o3.Irreps('32x0e+16x1e+8x2e') #o3.Irreps('128x0e+64x1e+32x2e')\n",
    "irreps_sh = o3.Irreps('1x0e+1x1e+1x2e')\n",
    "fc_neurons = [128, 64, 64]\n",
    "num_heads = 4\n",
    "alpha_drop = 0.2\n",
    "proj_drop = 0.0\n",
    "drop_path_rate = 0.0\n",
    "irreps_mlp_mid = 2\n",
    "n_scales = 4\n",
    "pool_ratio = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edf = EDF(irreps_input = irreps_input,\n",
    "          irreps_emb_init = irreps_node_embedding,\n",
    "          irreps_sh = irreps_sh,\n",
    "          fc_neurons_init = [32, 16, 16],\n",
    "          num_heads = 4,\n",
    "          n_scales = 4,\n",
    "          pool_ratio = 0.5,\n",
    "          dim_mult = [1, 2, 3, 4],\n",
    "          n_layers = 2,\n",
    "          gnn_radius = 2.0,\n",
    "          cutoff_radius = 3.0,\n",
    "          deterministic = True,\n",
    "          compile_head=True)\n",
    "\n",
    "edf = edf.to(device)\n",
    "if eval:\n",
    "    edf = edf.eval()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_list: List[DemoSequence] = load_demos(dir='demo/test_demo')\n",
    "demo_seq: DemoSequence = demo_list[0]\n",
    "\n",
    "demo: TargetPoseDemo = demo_seq[1]\n",
    "print(demo)\n",
    "\n",
    "scene_raw: PointCloud = demo.scene_pc\n",
    "grasp_raw: PointCloud = demo.grasp_pc\n",
    "target_poses: TargetPoseDemo = demo.target_poses\n",
    "\n",
    "scene_proc = scene_proc_fn(scene_raw).to(device)\n",
    "grasp_proc = grasp_proc_fn(grasp_raw).to(device)\n",
    "\n",
    "#print(scene_proc)\n",
    "#go.Figure(scene_unproc_fn(scene_proc).plotly())\n",
    "\n",
    "\n",
    "node_feature = scene_proc.colors\n",
    "node_coord = scene_proc.points\n",
    "batch = torch.zeros(len(node_coord), device=device, dtype=torch.long)\n",
    "\n",
    "query_coord = torch.randn(3,3, device=device)\n",
    "query_batch = torch.zeros_like(query_coord[...,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_val, edf_info = edf.forward(query_coord=query_coord, query_batch=query_batch, node_feature=node_feature, node_coord=node_coord, batch=batch)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rotate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusion_edf.quaternion_utils import quaternion_apply, random_quaternions\n",
    "rot = random_quaternions(1, device=device)\n",
    "rot = rot/rot.norm(dim=-1, keepdim=True)\n",
    "trans = torch.randn(3,device=device)\n",
    "node_coord_rot = quaternion_apply(rot, node_coord) + trans\n",
    "query_coord_rot = quaternion_apply(rot, query_coord) + trans\n",
    "\n",
    "node_feature_rot = node_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_val_rot, edf_info_rot = edf.forward(query_coord=query_coord_rot, query_batch=query_batch, node_feature=node_feature_rot, node_coord=node_coord_rot, batch=batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irrep_transform = e3nn_script(TransformFeatureQuaternion(irreps = o3.Irreps(edf.irreps_emb), device=device))\n",
    "a = field_val_rot\n",
    "b = field_val\n",
    "isclose = torch.isclose(irrep_transform(b, rot)[0], a, atol=0.01, rtol=0.01)\n",
    "# print(isclose)\n",
    "print(f\"Equivariance ratio: {(isclose.sum() / len(a.view(-1))).item()}\") # Slight non-equivariance comes from FPS downsampling algorithm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diff_edf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "79a0085b6cf04e1cff261ad12d41cff4e1530d9e68d1f8fc6bd159a2915452c6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
